#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollamaå®‰è£…å’Œè®¾ç½®è„šæœ¬
å¸®åŠ©ç”¨æˆ·å¿«é€Ÿè®¾ç½®Ollamaç¯å¢ƒå¹¶ä¸‹è½½æ‰€éœ€æ¨¡å‹
"""

import os
import sys
import subprocess
import platform
import requests
import json
import time

def check_system():
    """æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ"""
    system = platform.system().lower()
    print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ°ç³»ç»Ÿ: {platform.system()} {platform.release()}")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = sys.version_info
    print(f"ğŸ Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    if python_version < (3, 7):
        print("âš ï¸  å»ºè®®ä½¿ç”¨Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
    
    return system

def check_ollama_installed():
    """æ£€æŸ¥Ollamaæ˜¯å¦å·²å®‰è£…"""
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            version = result.stdout.strip()
            print(f"âœ… Ollamaå·²å®‰è£…: {version}")
            return True
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass
    
    print("âŒ Ollamaæœªå®‰è£…")
    return False

def install_ollama():
    """å®‰è£…Ollama"""
    system = platform.system().lower()
    
    print("ğŸ“¥ æ­£åœ¨å®‰è£…Ollama...")
    
    if system == "linux" or system == "darwin":  # Linuxæˆ–macOS
        try:
            # ä½¿ç”¨å®˜æ–¹å®‰è£…è„šæœ¬
            install_cmd = "curl -fsSL https://ollama.ai/install.sh | sh"
            print(f"æ‰§è¡Œå‘½ä»¤: {install_cmd}")
            
            result = subprocess.run(install_cmd, shell=True, check=True)
            if result.returncode == 0:
                print("âœ… Ollamaå®‰è£…æˆåŠŸ")
                return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ å®‰è£…å¤±è´¥: {e}")
            
    elif system == "windows":
        print("ğŸªŸ Windowsç³»ç»Ÿå®‰è£…è¯´æ˜:")
        print("1. è®¿é—® https://ollama.ai/download")
        print("2. ä¸‹è½½Windowså®‰è£…åŒ…")
        print("3. è¿è¡Œå®‰è£…ç¨‹åº")
        print("4. é‡å¯ç»ˆç«¯åå†è¿è¡Œæ­¤è„šæœ¬")
        
    else:
        print(f"âŒ ä¸æ”¯æŒçš„ç³»ç»Ÿ: {system}")
    
    return False

def check_ollama_service():
    """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ")
            return True
    except requests.exceptions.RequestException:
        pass
    
    print("âŒ OllamaæœåŠ¡æœªè¿è¡Œ")
    return False

def start_ollama_service():
    """å¯åŠ¨OllamaæœåŠ¡"""
    print("ğŸš€ æ­£åœ¨å¯åŠ¨OllamaæœåŠ¡...")
    
    try:
        # åœ¨åå°å¯åŠ¨OllamaæœåŠ¡
        if platform.system().lower() == "windows":
            subprocess.Popen(['ollama', 'serve'], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
        
        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        print("â³ ç­‰å¾…æœåŠ¡å¯åŠ¨...")
        for i in range(30):  # æœ€å¤šç­‰å¾…30ç§’
            time.sleep(1)
            if check_ollama_service():
                return True
            print(f"   ç­‰å¾…ä¸­... ({i+1}/30)")
        
        print("âŒ æœåŠ¡å¯åŠ¨è¶…æ—¶")
        return False
        
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ°ollamaå‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…")
        return False

def list_available_models():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            if models:
                print(f"ğŸ“š å·²å®‰è£…çš„æ¨¡å‹ ({len(models)}ä¸ª):")
                for model in models:
                    name = model.get('name', 'Unknown')
                    size = model.get('size', 0)
                    size_gb = size / (1024**3) if size > 0 else 0
                    print(f"   â€¢ {name} ({size_gb:.1f}GB)")
            else:
                print("ğŸ“š æœªæ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹")
            
            return [model.get('name', '') for model in models]
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
    
    return []

def download_model(model_name):
    """ä¸‹è½½æŒ‡å®šæ¨¡å‹"""
    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹: {model_name}")
    print("â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    try:
        # ä½¿ç”¨ollama pullå‘½ä»¤ä¸‹è½½æ¨¡å‹
        process = subprocess.Popen(['ollama', 'pull', model_name], 
                                 stdout=subprocess.PIPE, 
                                 stderr=subprocess.STDOUT, 
                                 text=True, universal_newlines=True)
        
        # å®æ—¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        for line in process.stdout:
            line = line.strip()
            if line:
                print(f"   {line}")
        
        process.wait()
        
        if process.returncode == 0:
            print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
            return True
        else:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸‹è½½å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def test_model(model_name):
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
    
    test_prompt = "Hello, how are you?"
    
    try:
        data = {
            "model": model_name,
            "prompt": test_prompt,
            "stream": False
        }
        
        response = requests.post("http://localhost:11434/api/generate", 
                               json=data, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            answer = result.get('response', '')
            if answer:
                print(f"âœ… æ¨¡å‹å“åº”æ­£å¸¸")
                print(f"   æµ‹è¯•é—®é¢˜: {test_prompt}")
                print(f"   æ¨¡å‹å›ç­”: {answer[:100]}{'...' if len(answer) > 100 else ''}")
                return True
        
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {response.status_code}")
        return False
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def install_python_dependencies():
    """å®‰è£…Pythonä¾èµ–"""
    dependencies = ['ollama', 'requests']
    
    print("ğŸ“¦ å®‰è£…Pythonä¾èµ–åŒ…...")
    
    for package in dependencies:
        try:
            print(f"   å®‰è£… {package}...")
            subprocess.run([sys.executable, '-m', 'pip', 'install', package], 
                         check=True, capture_output=True)
            print(f"   âœ… {package} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError as e:
            print(f"   âŒ {package} å®‰è£…å¤±è´¥: {e}")
            return False
    
    return True

def setup_medical_models():
    """è®¾ç½®åŒ»å­¦ç›¸å…³çš„æ¨èæ¨¡å‹"""
    recommended_models = {
        'llama2': {
            'name': 'llama2',
            'description': 'é€šç”¨å¤§æ¨¡å‹ï¼Œé€‚åˆåŒ»å­¦æŠ¥å‘Šç”Ÿæˆ',
            'size': '3.8GB',
            'recommended': True
        },
        'mistral': {
            'name': 'mistral',
            'description': 'é«˜æ•ˆæ¨¡å‹ï¼Œå“åº”é€Ÿåº¦å¿«',
            'size': '4.1GB', 
            'recommended': True
        },
        'codellama': {
            'name': 'codellama',
            'description': 'ä»£ç ç”Ÿæˆä¸“ç”¨ï¼Œé€‚åˆæŠ€æœ¯æ–‡æ¡£',
            'size': '3.8GB',
            'recommended': False
        },
        'llama2:13b': {
            'name': 'llama2:13b',
            'description': 'æ›´å¤§çš„æ¨¡å‹ï¼Œè´¨é‡æ›´é«˜ä½†éœ€è¦æ›´å¤šèµ„æº',
            'size': '7.3GB',
            'recommended': False
        }
    }
    
    print("\nğŸ¥ åŒ»å­¦AIæ¨èæ¨¡å‹:")
    for key, model in recommended_models.items():
        status = "ğŸŒŸ æ¨è" if model['recommended'] else "ğŸ”§ å¯é€‰"
        print(f"   {status} {model['name']} - {model['description']} ({model['size']})")
    
    installed_models = list_available_models()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ¨èæ¨¡å‹å·²å®‰è£…
    has_recommended = any(model['name'] in installed_models for model in recommended_models.values() if model['recommended'])
    
    if not has_recommended:
        print("\nğŸ“¥ å»ºè®®å®‰è£…è‡³å°‘ä¸€ä¸ªæ¨èæ¨¡å‹:")
        
        choice = input("æ˜¯å¦å®‰è£… llama2 æ¨¡å‹? (y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            if download_model('llama2'):
                test_model('llama2')
        
        choice = input("æ˜¯å¦å®‰è£… mistral æ¨¡å‹? (y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            if download_model('mistral'):
                test_model('mistral')
    else:
        print("âœ… å·²æœ‰æ¨èæ¨¡å‹å®‰è£…")

def create_usage_examples():
    """åˆ›å»ºä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶"""
    examples_content = """
# Ollama + èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ç±» ä½¿ç”¨ç¤ºä¾‹

## åŸºæœ¬ä½¿ç”¨

### 1. ç¡®ä¿OllamaæœåŠ¡è¿è¡Œ
```bash
ollama serve
```

### 2. åŸºæœ¬çš„å¤šæ¨¡æ€åˆ†æ
```bash
python multimodal_service.py --image ../../data/ChestXRay/test/PNEUMONIA/person1_virus_11.jpeg
```

### 3. ä½¿ç”¨ä¸åŒçš„LLMæ¨¡å‹
```bash
python multimodal_service.py --image path/to/xray.jpg --llm mistral
python multimodal_service.py --image path/to/xray.jpg --llm llama2
```

### 4. ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
```bash
python multimodal_service.py --image path/to/xray.jpg --simple
```

### 5. åªæ˜¾ç¤ºæ€»ç»“
```bash
python multimodal_service.py --image path/to/xray.jpg --summary-only
```

### 6. ä¿å­˜å®Œæ•´æŠ¥å‘Š
```bash
python multimodal_service.py --image path/to/xray.jpg --output reports/analysis.json
```

## APIä½¿ç”¨ç¤ºä¾‹

### Python APIè°ƒç”¨
```python
from multimodal_service import MedicalMultimodalAI

# åˆ›å»ºAIç³»ç»Ÿ
ai = MedicalMultimodalAI('checkpoints/best_model.pth', 'llama2')

# åˆ†æå›¾åƒ
result = ai.analyze_xray_with_report('path/to/xray.jpg')

# ç”Ÿæˆæ€»ç»“
summary = ai.generate_summary_report(result)
print(summary)
```

## å¸¸è§é—®é¢˜

### Q: OllamaæœåŠ¡æ— æ³•å¯åŠ¨ï¼Ÿ
A: 
1. ç¡®ä¿å·²æ­£ç¡®å®‰è£…Ollama
2. æ£€æŸ¥ç«¯å£11434æ˜¯å¦è¢«å ç”¨
3. é‡å¯ç»ˆç«¯åé‡è¯•

### Q: æ¨¡å‹ä¸‹è½½å¾ˆæ…¢ï¼Ÿ
A: 
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. è€ƒè™‘ä½¿ç”¨é•œåƒæº
3. å¯ä»¥å°è¯•è¾ƒå°çš„æ¨¡å‹å¦‚mistral

### Q: å†…å­˜ä¸è¶³ï¼Ÿ
A: 
1. å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾å†…å­˜
2. ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
3. è€ƒè™‘å¢åŠ ç³»ç»Ÿå†…å­˜

### Q: ç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡ä¸ä½³ï¼Ÿ
A: 
1. å°è¯•ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å¦‚llama2:13b
2. è°ƒæ•´æç¤ºè¯­æ¨¡æ¿
3. ç¡®ä¿å›¾åƒåˆ†ç±»ç»“æœå‡†ç¡®
"""
    
    with open('ollama_usage_examples.md', 'w', encoding='utf-8') as f:
        f.write(examples_content)
    
    print(f"ğŸ“– ä½¿ç”¨ç¤ºä¾‹å·²ä¿å­˜åˆ°: ollama_usage_examples.md")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Ollama + èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ç±» ç¯å¢ƒè®¾ç½®")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ç³»ç»Ÿ
    system = check_system()
    
    # 2. æ£€æŸ¥å¹¶å®‰è£…Ollama
    if not check_ollama_installed():
        print("\nğŸ“¥ éœ€è¦å®‰è£…Ollama")
        choice = input("æ˜¯å¦ç°åœ¨å®‰è£…? (y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            if not install_ollama():
                print("âŒ å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…Ollama")
                return
        else:
            print("â­ï¸  è·³è¿‡å®‰è£…ï¼Œè¯·æ‰‹åŠ¨å®‰è£…åé‡æ–°è¿è¡Œ")
            return
    
    # 3. å®‰è£…Pythonä¾èµ–
    print("\nğŸ“¦ æ£€æŸ¥Pythonä¾èµ–...")
    if not install_python_dependencies():
        print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")
        return
    
    # 4. å¯åŠ¨OllamaæœåŠ¡
    print("\nğŸš€ æ£€æŸ¥OllamaæœåŠ¡...")
    if not check_ollama_service():
        choice = input("OllamaæœåŠ¡æœªè¿è¡Œï¼Œæ˜¯å¦å¯åŠ¨? (y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            if not start_ollama_service():
                print("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")
                print("ğŸ’¡ è¯·æ‰‹åŠ¨è¿è¡Œ: ollama serve")
                return
        else:
            print("â­ï¸  è·³è¿‡æœåŠ¡å¯åŠ¨")
            print("ğŸ’¡ ä½¿ç”¨å‰è¯·æ‰‹åŠ¨è¿è¡Œ: ollama serve")
    
    # 5. è®¾ç½®åŒ»å­¦æ¨¡å‹
    print("\nğŸ¥ è®¾ç½®åŒ»å­¦æ¨¡å‹...")
    setup_medical_models()
    
    # 6. åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
    print("\nğŸ“– åˆ›å»ºä½¿ç”¨ç¤ºä¾‹...")
    create_usage_examples()
    
    # 7. å®Œæˆè®¾ç½®
    print("\n" + "=" * 50)
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    print("\nğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. ç¡®ä¿OllamaæœåŠ¡è¿è¡Œ: ollama serve")
    print("2. æµ‹è¯•å¤šæ¨¡æ€åˆ†æ:")
    print("   python multimodal_service.py --image path/to/xray.jpg")
    print("3. æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨æ–¹æ³•: ollama_usage_examples.md")
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nğŸ§ª è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
    if check_ollama_service():
        models = list_available_models()
        if models:
            test_model_name = models[0].split(':')[0]
            print(f"æµ‹è¯•æ¨¡å‹: {test_model_name}")
            test_model(test_model_name)
    
    print("\nğŸ‰ è®¾ç½®å®Œæˆï¼å¼€å§‹ä½¿ç”¨ä½ çš„å¤šæ¨¡æ€åŒ»å­¦AIç³»ç»Ÿå§ï¼")

if __name__ == "__main__":
    main() 