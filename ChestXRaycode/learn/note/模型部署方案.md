# æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆè¯¦è§£

## ğŸ“‹ ç›®å½•
- [Ollamaä¸å›¾åƒæ¨¡å‹çš„å…³ç³»](#ollamaä¸å›¾åƒæ¨¡å‹çš„å…³ç³»)
- [ç›´æ¥éƒ¨ç½²æ–¹æ¡ˆ](#ç›´æ¥éƒ¨ç½²æ–¹æ¡ˆ)
- [å¤šæ¨¡æ€AIç³»ç»Ÿ](#å¤šæ¨¡æ€aiç³»ç»Ÿ)
- [ä¸Ollamaé›†æˆæ–¹æ¡ˆ](#ä¸ollamaé›†æˆæ–¹æ¡ˆ)
- [å…¶ä»–éƒ¨ç½²é€‰é¡¹](#å…¶ä»–éƒ¨ç½²é€‰é¡¹)
- [å®æ–½æ­¥éª¤](#å®æ–½æ­¥éª¤)

---

## Ollamaä¸å›¾åƒæ¨¡å‹çš„å…³ç³»

### ğŸ¤” é‡è¦æ¾„æ¸…

**Ollamaçš„ä¸»è¦ç”¨é€”**ï¼š
- Ollamaæ˜¯ç”¨äºè¿è¡Œ**å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**çš„å·¥å…·
- æ”¯æŒçš„æ¨¡å‹ï¼šLlamaã€Mistralã€CodeLlamaã€Vicunaç­‰
- ä¸»è¦å¤„ç†ï¼šæ–‡æœ¬è¾“å…¥â†’æ–‡æœ¬è¾“å‡º

**ä½ çš„æ¨¡å‹ç±»å‹**ï¼š
- èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ç±»å™¨æ˜¯**å›¾åƒåˆ†ç±»æ¨¡å‹**
- åŸºäºPyTorch + ResNetæ¶æ„
- å¤„ç†ï¼šå›¾åƒè¾“å…¥â†’åˆ†ç±»ç»“æœ

### ğŸ’¡ å¯èƒ½çš„é›†æˆåœºæ™¯

è™½ç„¶ä¸èƒ½ç›´æ¥å°†å›¾åƒåˆ†ç±»æ¨¡å‹å¯¼å…¥Ollamaï¼Œä½†å¯ä»¥åˆ›å»ºé›†æˆç³»ç»Ÿï¼š

```
ç”¨æˆ·ä¸Šä¼ Xå…‰ç‰‡ â†’ å›¾åƒåˆ†ç±»æ¨¡å‹é¢„æµ‹ â†’ ç»“æœä¼ ç»™LLM â†’ ç”ŸæˆåŒ»å­¦æŠ¥å‘Š
```

---

## ç›´æ¥éƒ¨ç½²æ–¹æ¡ˆ

### 1. **REST APIéƒ¨ç½²** (æ¨è)

åˆ›å»ºä¸€ä¸ªWeb APIæœåŠ¡æ¥éƒ¨ç½²ä½ çš„æ¨¡å‹ï¼š

```python
# æ–‡ä»¶å: ChestXRay/learn/api_server.py
from flask import Flask, request, jsonify
from deploy_simple import ChestXRayPredictor
import base64
import io
from PIL import Image

app = Flask(__name__)
predictor = ChestXRayPredictor('checkpoints/best_model.pth')

@app.route('/predict', methods=['POST'])
def predict():
    # æ¥æ”¶å›¾ç‰‡å¹¶é¢„æµ‹
    image_data = request.json['image']
    # å¤„ç†base64å›¾ç‰‡
    image = Image.open(io.BytesIO(base64.b64decode(image_data)))
    result = predictor.predict_single_image(image)
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 2. **FastAPIéƒ¨ç½²** (é«˜æ€§èƒ½)

```python
# æ–‡ä»¶å: ChestXRay/learn/fastapi_server.py
from fastapi import FastAPI, File, UploadFile
from deploy_simple import ChestXRayPredictor
import uvicorn

app = FastAPI(title="èƒ¸éƒ¨Xå…‰ç‰‡åˆ†ç±»API")
predictor = ChestXRayPredictor('checkpoints/best_model.pth')

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    contents = await file.read()
    result = predictor.predict_single_image(io.BytesIO(contents))
    return result

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 3. **Dockerå®¹å™¨åŒ–**

```dockerfile
# Dockerfile
FROM pytorch/pytorch:latest

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "fastapi_server.py"]
```

---

## å¤šæ¨¡æ€AIç³»ç»Ÿ

### ğŸ¯ ç›®æ ‡ï¼šå›¾åƒåˆ†æ + æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ

åˆ›å»ºä¸€ä¸ªç»“åˆå›¾åƒåˆ†ç±»å’Œè¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿï¼š

```
Xå…‰ç‰‡ â†’ å›¾åƒåˆ†ç±» â†’ ç»“æ„åŒ–ç»“æœ â†’ LLM â†’ è¯¦ç»†åŒ»å­¦æŠ¥å‘Š
```

### æ¶æ„è®¾è®¡

```python
# ç³»ç»Ÿæ¶æ„ç¤ºæ„
class MultimodalMedicalAI:
    def __init__(self):
        self.image_classifier = ChestXRayPredictor('checkpoints/best_model.pth')
        self.llm_client = OllamaClient()  # è¿æ¥Ollama
    
    def analyze_xray(self, image_path):
        # 1. å›¾åƒåˆ†ç±»
        classification_result = self.image_classifier.predict_single_image(image_path)
        
        # 2. æ„å»ºLLMæç¤º
        prompt = self.build_medical_prompt(classification_result)
        
        # 3. LLMç”ŸæˆæŠ¥å‘Š
        medical_report = self.llm_client.generate(prompt)
        
        return {
            'classification': classification_result,
            'medical_report': medical_report
        }
```

---

## ä¸Ollamaé›†æˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šAPIæ¥å£é›†æˆ

**æ­¥éª¤**ï¼š
1. éƒ¨ç½²å›¾åƒåˆ†ç±»æ¨¡å‹ä¸ºAPIæœåŠ¡
2. ä½¿ç”¨Ollamaè¿è¡ŒåŒ»å­¦é¢†åŸŸçš„LLM
3. åˆ›å»ºé›†æˆæœåŠ¡è¿æ¥ä¸¤è€…

```python
# é›†æˆæœåŠ¡ç¤ºä¾‹
import requests
import json

class XRayOllamaIntegration:
    def __init__(self):
        self.xray_api = "http://localhost:8000"  # å›¾åƒåˆ†ç±»API
        self.ollama_api = "http://localhost:11434"  # Ollama API
    
    def analyze_and_report(self, image_path):
        # 1. å›¾åƒåˆ†ç±»
        with open(image_path, 'rb') as f:
            files = {'file': f}
            classification = requests.post(f"{self.xray_api}/predict", files=files).json()
        
        # 2. æ„å»ºåŒ»å­¦æç¤º
        prompt = f"""
        ä½œä¸ºä¸€åæ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè¯·æ ¹æ®ä»¥ä¸‹AIåˆ†æç»“æœç”Ÿæˆè¯¦ç»†çš„åŒ»å­¦æŠ¥å‘Šï¼š
        
        åˆ†ç±»ç»“æœï¼š{classification['predicted_class']}
        ç½®ä¿¡åº¦ï¼š{classification['confidence']:.2%}
        æ¦‚ç‡åˆ†å¸ƒï¼š{classification['probabilities']}
        
        è¯·æä¾›ï¼š
        1. å½±åƒå­¦æè¿°
        2. è¯Šæ–­æ„è§  
        3. å»ºè®®çš„åç»­æ£€æŸ¥
        4. æ³¨æ„äº‹é¡¹
        """
        
        # 3. LLMç”ŸæˆæŠ¥å‘Š
        ollama_data = {
            "model": "llama2",  # æˆ–å…¶ä»–åŒ»å­¦æ¨¡å‹
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(f"{self.ollama_api}/api/generate", json=ollama_data)
        medical_report = response.json()['response']
        
        return {
            'classification_result': classification,
            'medical_report': medical_report
        }
```

### æ–¹æ¡ˆ2ï¼šæœ¬åœ°é›†æˆæœåŠ¡

```python
# æ–‡ä»¶å: ChestXRay/learn/multimodal_service.py
import ollama
from deploy_simple import ChestXRayPredictor
import json

class MedicalMultimodalAI:
    def __init__(self, model_path, llm_model="llama2"):
        # åˆå§‹åŒ–å›¾åƒåˆ†ç±»å™¨
        self.predictor = ChestXRayPredictor(model_path)
        self.llm_model = llm_model
        
        # æ£€æŸ¥Ollamaè¿æ¥
        try:
            ollama.list()
            print("âœ… Ollamaè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
    
    def create_medical_prompt(self, classification_result):
        """åˆ›å»ºåŒ»å­¦æŠ¥å‘Šç”Ÿæˆæç¤º"""
        if 'error' in classification_result:
            return f"å›¾åƒåˆ†æå¤±è´¥ï¼š{classification_result['error']}"
        
        prompt = f"""
ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·æ ¹æ®AIè¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„èƒ¸éƒ¨Xå…‰ç‰‡è¯Šæ–­æŠ¥å‘Šã€‚

AIåˆ†æç»“æœï¼š
- é¢„æµ‹ç±»åˆ«ï¼š{classification_result['predicted_class']}
- ç½®ä¿¡åº¦ï¼š{classification_result['confidence']:.1%}
- å„ç±»åˆ«æ¦‚ç‡ï¼š
  * æ­£å¸¸ (NORMAL): {classification_result['probabilities']['NORMAL']:.1%}
  * è‚ºç‚ (PNEUMONIA): {classification_result['probabilities']['PNEUMONIA']:.1%}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼ç”ŸæˆæŠ¥å‘Šï¼š

## èƒ¸éƒ¨Xå…‰ç‰‡è¯Šæ–­æŠ¥å‘Š

**æ‚£è€…ä¿¡æ¯**: [AIè¾…åŠ©è¯Šæ–­]
**æ£€æŸ¥æ—¥æœŸ**: {classification_result['prediction_time'][:10]}

**å½±åƒå­¦æè¿°**:
[åŸºäºAIåˆ†æç»“æœæè¿°å½±åƒç‰¹å¾]

**è¯Šæ–­æ„è§**:
[åŸºäºåˆ†ç±»ç»“æœç»™å‡ºè¯Šæ–­æ„è§]

**å»ºè®®**:
[æ ¹æ®è¯Šæ–­ç»“æœæä¾›åŒ»å­¦å»ºè®®]

**å¤‡æ³¨**:
- æœ¬æŠ¥å‘ŠåŸºäºAIè¾…åŠ©åˆ†æï¼Œä»…ä¾›ä¸´åºŠå‚è€ƒ
- æœ€ç»ˆè¯Šæ–­éœ€ç»“åˆä¸´åºŠè¡¨ç°å’Œå…¶ä»–æ£€æŸ¥
- å¦‚æœ‰ç–‘é—®è¯·å’¨è¯¢ä¸“ç§‘åŒ»ç”Ÿ

è¯·ç¡®ä¿æŠ¥å‘Šä¸“ä¸šã€å‡†ç¡®ã€ç¬¦åˆåŒ»å­¦æ ‡å‡†ã€‚
"""
        return prompt
    
    def analyze_xray_with_report(self, image_path):
        """å®Œæ•´çš„Xå…‰ç‰‡åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ"""
        print(f"ğŸ” æ­£åœ¨åˆ†æXå…‰ç‰‡: {image_path}")
        
        # 1. å›¾åƒåˆ†ç±»
        classification = self.predictor.predict_single_image(image_path)
        
        if 'error' in classification:
            return {
                'error': classification['error'],
                'image_path': image_path
            }
        
        print(f"ğŸ“Š åˆ†ç±»å®Œæˆ: {classification['predicted_class']} (ç½®ä¿¡åº¦: {classification['confidence']:.1%})")
        
        # 2. ç”ŸæˆåŒ»å­¦æŠ¥å‘Š
        prompt = self.create_medical_prompt(classification)
        
        try:
            print("ğŸ“ æ­£åœ¨ç”ŸæˆåŒ»å­¦æŠ¥å‘Š...")
            response = ollama.generate(
                model=self.llm_model,
                prompt=prompt
            )
            medical_report = response['response']
            print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        except Exception as e:
            medical_report = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        
        return {
            'image_analysis': classification,
            'medical_report': medical_report,
            'combined_assessment': {
                'risk_level': self._assess_risk_level(classification),
                'confidence_interpretation': self._interpret_confidence(classification['confidence']),
                'recommendations': self._generate_recommendations(classification)
            }
        }
    
    def _assess_risk_level(self, classification):
        """è¯„ä¼°é£é™©ç­‰çº§"""
        if classification['predicted_class'] == 'PNEUMONIA':
            if classification['confidence'] >= 0.9:
                return "é«˜é£é™© - å¼ºçƒˆå»ºè®®ç«‹å³å°±åŒ»"
            elif classification['confidence'] >= 0.7:
                return "ä¸­é£é™© - å»ºè®®å°½å¿«å°±åŒ»"
            else:
                return "ä½é£é™© - å»ºè®®åŒ»ç–—å¤æŸ¥"
        else:
            if classification['confidence'] >= 0.9:
                return "æ­£å¸¸ - æ— æ˜æ˜¾å¼‚å¸¸"
            else:
                return "ä¸ç¡®å®š - å»ºè®®ä¸“ä¸šè¯„ä¼°"
    
    def _interpret_confidence(self, confidence):
        """è§£é‡Šç½®ä¿¡åº¦"""
        if confidence >= 0.95:
            return "AIæ¨¡å‹å¯¹æ­¤è¯Šæ–­éå¸¸ç¡®ä¿¡"
        elif confidence >= 0.85:
            return "AIæ¨¡å‹å¯¹æ­¤è¯Šæ–­æ¯”è¾ƒç¡®ä¿¡"
        elif confidence >= 0.7:
            return "AIæ¨¡å‹å¯¹æ­¤è¯Šæ–­æœ‰ä¸€å®šæŠŠæ¡"
        else:
            return "AIæ¨¡å‹å¯¹æ­¤è¯Šæ–­ä¸å¤Ÿç¡®å®šï¼Œå»ºè®®äººå·¥å¤æŸ¥"
    
    def _generate_recommendations(self, classification):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = [
            "æœ¬ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç”Ÿè¯Šæ–­",
            "å¦‚æœ‰ç—‡çŠ¶æˆ–æ‹…å¿§ï¼Œè¯·åŠæ—¶å°±åŒ»"
        ]
        
        if classification['predicted_class'] == 'PNEUMONIA':
            if classification['confidence'] >= 0.8:
                recommendations.extend([
                    "å»ºè®®è¿›è¡Œè¡€å¸¸è§„æ£€æŸ¥",
                    "å¯è€ƒè™‘ç—°åŸ¹å…»æ£€æŸ¥",
                    "å¿…è¦æ—¶è¿›è¡ŒCTæ£€æŸ¥"
                ])
            else:
                recommendations.extend([
                    "å»ºè®®å¯†åˆ‡è§‚å¯Ÿç—‡çŠ¶å˜åŒ–",
                    "å¦‚å‡ºç°å‘çƒ­ã€å’³å—½åŠ é‡ç­‰ç—‡çŠ¶è¯·åŠæ—¶å°±åŒ»"
                ])
        
        return recommendations

def main():
    """æ¼”ç¤ºå¤šæ¨¡æ€åŒ»å­¦AIç³»ç»Ÿ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€åŒ»å­¦AIè¯Šæ–­ç³»ç»Ÿ')
    parser.add_argument('--model', type=str, default='checkpoints/best_model.pth',
                       help='å›¾åƒåˆ†ç±»æ¨¡å‹è·¯å¾„')
    parser.add_argument('--image', type=str, required=True,
                       help='Xå…‰ç‰‡å›¾åƒè·¯å¾„')
    parser.add_argument('--llm', type=str, default='llama2',
                       help='Ollama LLMæ¨¡å‹åç§°')
    parser.add_argument('--output', type=str,
                       help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤šæ¨¡æ€AIç³»ç»Ÿ
    try:
        ai_system = MedicalMultimodalAI(args.model, args.llm)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ†æå›¾åƒå¹¶ç”ŸæˆæŠ¥å‘Š
    result = ai_system.analyze_xray_with_report(args.image)
    
    if 'error' in result:
        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        return
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*80)
    print("ğŸ¥ å¤šæ¨¡æ€åŒ»å­¦AIè¯Šæ–­ç»“æœ")
    print("="*80)
    
    print(f"\nğŸ“Š å›¾åƒåˆ†æç»“æœ:")
    print(f"   é¢„æµ‹ç±»åˆ«: {result['image_analysis']['predicted_class']}")
    print(f"   ç½®ä¿¡åº¦: {result['image_analysis']['confidence']:.1%}")
    print(f"   é£é™©è¯„ä¼°: {result['combined_assessment']['risk_level']}")
    
    print(f"\nğŸ“ AIç”Ÿæˆçš„åŒ»å­¦æŠ¥å‘Š:")
    print(result['medical_report'])
    
    print(f"\nğŸ’¡ ç³»ç»Ÿå»ºè®®:")
    for rec in result['combined_assessment']['recommendations']:
        print(f"   â€¢ {rec}")
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³: {args.output}")

if __name__ == "__main__":
    main()
```

---

## å…¶ä»–éƒ¨ç½²é€‰é¡¹

### 1. **ONNXæ ¼å¼è½¬æ¢**

å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œæé«˜å…¼å®¹æ€§ï¼š

```python
# è½¬æ¢è„šæœ¬
import torch
import torch.onnx
from model import create_model

def convert_to_onnx(model_path, onnx_path):
    # åŠ è½½æ¨¡å‹
    model = create_model(num_classes=2, model_name='resnet50')
    checkpoint = torch.load(model_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, 3, 224, 224)
    
    # è½¬æ¢ä¸ºONNX
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output']
    )
```

### 2. **TorchScriptåºåˆ—åŒ–**

```python
# åºåˆ—åŒ–ä¸ºTorchScript
def convert_to_torchscript(model_path, script_path):
    model = create_model(num_classes=2, model_name='resnet50')
    checkpoint = torch.load(model_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # è½¬æ¢ä¸ºTorchScript
    traced_model = torch.jit.trace(model, torch.randn(1, 3, 224, 224))
    traced_model.save(script_path)
```

### 3. **äº‘ç«¯éƒ¨ç½²**

```yaml
# Kuberneteséƒ¨ç½²é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chest-xray-classifier
spec:
  replicas: 3
  selector:
    matchLabels:
      app: chest-xray-classifier
  template:
    metadata:
      labels:
        app: chest-xray-classifier
    spec:
      containers:
      - name: classifier
        image: your-registry/chest-xray-classifier:latest
        ports:
        - containerPort: 8000
```

---

## å®æ–½æ­¥éª¤

### ğŸš€ å¿«é€Ÿå¼€å§‹ï¼šåˆ›å»ºå¤šæ¨¡æ€AIç³»ç»Ÿ

#### æ­¥éª¤1ï¼šå®‰è£…Ollama

```bash
# å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# ä¸‹è½½åŒ»å­¦ç›¸å…³æ¨¡å‹ï¼ˆåœ¨æ–°ç»ˆç«¯ï¼‰
ollama pull llama2  # æˆ–è€…å…¶ä»–é€‚åˆçš„æ¨¡å‹
ollama pull codellama  # ç”¨äºä»£ç ç”Ÿæˆ
ollama pull mistral   # æ›´å¿«çš„æ¨¡å‹
```

#### æ­¥éª¤2ï¼šå®‰è£…Pythonä¾èµ–

```bash
cd ChestXRay/learn
pip install ollama flask fastapi uvicorn
```

#### æ­¥éª¤3ï¼šåˆ›å»ºå¤šæ¨¡æ€æœåŠ¡

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº†å®Œæ•´çš„ä»£ç ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š

```bash
# æµ‹è¯•å¤šæ¨¡æ€AIç³»ç»Ÿ
python multimodal_service.py --image ../../data/ChestXRay/test/PNEUMONIA/person1_virus_11.jpeg --output report.json
```

#### æ­¥éª¤4ï¼šéƒ¨ç½²ä¸ºWebæœåŠ¡

```python
# åˆ›å»ºWebç•Œé¢ï¼ˆå¯é€‰ï¼‰
from flask import Flask, render_template, request, jsonify
from multimodal_service import MedicalMultimodalAI

app = Flask(__name__)
ai_system = MedicalMultimodalAI('checkpoints/best_model.pth')

@app.route('/')
def index():
    return render_template('upload.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    file = request.files['xray']
    result = ai_system.analyze_xray_with_report(file)
    return jsonify(result)
```

### ğŸ¯ é€‰æ‹©æœ€é€‚åˆçš„æ–¹æ¡ˆ

**æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©**ï¼š

1. **åªéœ€è¦å›¾åƒåˆ†ç±»** â†’ ä½¿ç”¨ `deploy_simple.py`
2. **éœ€è¦æ™ºèƒ½åŒ»å­¦æŠ¥å‘Š** â†’ ä½¿ç”¨å¤šæ¨¡æ€AIç³»ç»Ÿ
3. **éœ€è¦WebæœåŠ¡** â†’ éƒ¨ç½²FastAPIæˆ–Flask
4. **éœ€è¦ç”Ÿäº§ç¯å¢ƒ** â†’ Docker + Kubernetes

### ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®

1. **å…ˆæµ‹è¯•åŸºç¡€åŠŸèƒ½**ï¼š
   ```bash
   python deploy_simple.py --image path/to/xray.jpg
   ```

2. **ç„¶åå°è¯•å¤šæ¨¡æ€ç³»ç»Ÿ**ï¼š
   ```bash
   # ç¡®ä¿Ollamaè¿è¡Œ
   ollama serve
   
   # åœ¨æ–°ç»ˆç«¯è¿è¡Œ
   python multimodal_service.py --image path/to/xray.jpg
   ```

3. **æ ¹æ®æ•ˆæœå†³å®šè¿›ä¸€æ­¥å¼€å‘æ–¹å‘**

---

## æ€»ç»“

è™½ç„¶ä¸èƒ½ç›´æ¥å°†å›¾åƒåˆ†ç±»æ¨¡å‹å¯¼å…¥Ollamaï¼Œä½†å¯ä»¥åˆ›å»ºæ›´å¼ºå¤§çš„å¤šæ¨¡æ€AIç³»ç»Ÿï¼š

- **å›¾åƒåˆ†æ** + **æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ**
- **ä¸“ä¸šåŒ»å­¦å»ºè®®** + **é£é™©è¯„ä¼°**
- **å¯æ‰©å±•çš„éƒ¨ç½²æ–¹æ¡ˆ**

è¿™æ ·çš„ç³»ç»Ÿæ¯”å•çº¯çš„æ¨¡å‹æ›´æœ‰å®ç”¨ä»·å€¼ï¼ 