#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollamaè¿æ¥æµ‹è¯•è„šæœ¬
è¯Šæ–­å’Œä¿®å¤Ollamaè¿æ¥é—®é¢˜
"""

import sys
import json
import traceback
import subprocess

def test_ollama_import():
    """æµ‹è¯•ollamaåŒ…å¯¼å…¥"""
    print("ğŸ” æµ‹è¯•ollamaåŒ…å¯¼å…¥...")
    try:
        import ollama
        print("âœ… ollamaåŒ…å¯¼å…¥æˆåŠŸ")
        print(f"   ç‰ˆæœ¬ä¿¡æ¯: {getattr(ollama, '__version__', 'æœªçŸ¥')}")
        return True, ollama
    except ImportError as e:
        print(f"âŒ ollamaåŒ…å¯¼å…¥å¤±è´¥: {e}")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: pip install ollama")
        return False, None

def test_ollama_service():
    """æµ‹è¯•OllamaæœåŠ¡è¿æ¥"""
    print("\nğŸ” æµ‹è¯•OllamaæœåŠ¡è¿æ¥...")
    try:
        import ollama
        # å°è¯•åˆ—å‡ºæ¨¡å‹
        models_response = ollama.list()
        print("âœ… OllamaæœåŠ¡è¿æ¥æˆåŠŸ")
        
        if 'models' in models_response:
            models_list = models_response['models']
            available_models = []
            
            for model in models_list:
                # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„Ollamaå“åº”æ ¼å¼
                if hasattr(model, 'model'):
                    available_models.append(model.model)
                elif hasattr(model, 'name'):
                    available_models.append(model.name)
                elif isinstance(model, dict):
                    if 'model' in model:
                        available_models.append(model['model'])
                    elif 'name' in model:
                        available_models.append(model['name'])
                else:
                    # æ‰“å°æ¨¡å‹å¯¹è±¡ç»“æ„ç”¨äºè°ƒè¯•
                    print(f"   æ¨¡å‹å¯¹è±¡ç»“æ„: {dir(model)}")
                    available_models.append(str(model))
            
            if available_models:
                print(f"   å¯ç”¨æ¨¡å‹: {available_models}")
                return True, available_models
            else:
                print("   âš ï¸  æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹")
                print(f"   å“åº”ä¸­çš„æ¨¡å‹æ•°é‡: {len(models_list)}")
                if models_list:
                    print(f"   ç¬¬ä¸€ä¸ªæ¨¡å‹å¯¹è±¡: {models_list[0]}")
                return True, []  # æœåŠ¡æ­£å¸¸ï¼Œä½†æ²¡æœ‰æ¨¡å‹
        else:
            print("âš ï¸  å“åº”æ ¼å¼å¼‚å¸¸")
            print(f"   åŸå§‹å“åº”: {models_response}")
            return False, []
            
    except Exception as e:
        print(f"âŒ OllamaæœåŠ¡è¿æ¥å¤±è´¥: {e}")
        print("   é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        return False, []

def test_model_generation():
    """æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½...")
    try:
        import ollama
        
        # è·å–å¯ç”¨æ¨¡å‹ - ä½¿ç”¨ä¹‹å‰ä¿®å¤çš„é€»è¾‘
        models_response = ollama.list()
        if not models_response.get('models'):
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return False
        
        # è·å–æ¨¡å‹åç§°ï¼ˆä½¿ç”¨å…¼å®¹çš„æ–¹å¼ï¼‰
        models_list = models_response['models']
        model_name = None
        
        for model in models_list:
            if hasattr(model, 'model'):
                model_name = model.model
                break
            elif hasattr(model, 'name'):
                model_name = model.name
                break
            elif isinstance(model, dict):
                if 'model' in model:
                    model_name = model['model']
                    break
                elif 'name' in model:
                    model_name = model['name']
                    break
        
        if not model_name:
            print("âŒ æ— æ³•ç¡®å®šæ¨¡å‹åç§°")
            print(f"   ç¬¬ä¸€ä¸ªæ¨¡å‹å¯¹è±¡: {models_list[0]}")
            return False
        
        print(f"   ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        test_prompt = "Hello, this is a simple test. Please respond briefly."
        print(f"   æµ‹è¯•æç¤º: {test_prompt}")
        
        try:
            response = ollama.generate(
                model=model_name,
                prompt=test_prompt
            )
            
            if 'response' in response:
                print("âœ… æ¨¡å‹ç”Ÿæˆæµ‹è¯•æˆåŠŸ")
                print(f"   å“åº”: {response['response'][:100]}...")
                return True
            else:
                print("âŒ ç”Ÿæˆå“åº”æ ¼å¼å¼‚å¸¸")
                print(f"   åŸå§‹å“åº”: {response}")
                return False
        except Exception as gen_error:
            print(f"âŒ ç”Ÿæˆè¯·æ±‚å¤±è´¥: {gen_error}")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def check_ollama_process():
    """æ£€æŸ¥Ollamaè¿›ç¨‹çŠ¶æ€"""
    print("\nğŸ” æ£€æŸ¥Ollamaè¿›ç¨‹çŠ¶æ€...")
    try:
        if sys.platform.startswith('win'):
            # Windows
            result = subprocess.run(['tasklist', '/fi', 'imagename eq ollama.exe'], 
                                  capture_output=True, text=True)
            if 'ollama.exe' in result.stdout:
                print("âœ… Ollamaè¿›ç¨‹æ­£åœ¨è¿è¡Œ")
                return True
            else:
                print("âŒ Ollamaè¿›ç¨‹æœªè¿è¡Œ")
                return False
        else:
            # Linux/Mac
            result = subprocess.run(['pgrep', 'ollama'], capture_output=True)
            if result.returncode == 0:
                print("âœ… Ollamaè¿›ç¨‹æ­£åœ¨è¿è¡Œ")
                return True
            else:
                print("âŒ Ollamaè¿›ç¨‹æœªè¿è¡Œ")
                return False
    except Exception as e:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥è¿›ç¨‹çŠ¶æ€: {e}")
        return None

def auto_install_recommended_model():
    """è‡ªåŠ¨å®‰è£…æ¨èçš„åŒ»å­¦æ¨¡å‹"""
    print("\nğŸ”„ è‡ªåŠ¨å®‰è£…æ¨èæ¨¡å‹...")
    
    # æ¨èçš„æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    recommended_models = [
        ('llama3.1:8b', 'é«˜è´¨é‡é€šç”¨æ¨¡å‹ï¼Œé€‚åˆåŒ»å­¦åˆ†æ'),
        ('llama2:7b', 'ç¨³å®šçš„ä¸­å‹æ¨¡å‹'),
        ('llama2:13b', 'æ›´å¤§çš„æ¨¡å‹ï¼Œæ›´å¥½çš„æ€§èƒ½ï¼ˆéœ€è¦æ›´å¤šå†…å­˜ï¼‰'),
    ]
    
    try:
        import ollama
        
        for model_name, description in recommended_models:
            try:
                print(f"   æ­£åœ¨ä¸‹è½½ {model_name} ({description})...")
                print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                
                # å¼€å§‹ä¸‹è½½æ¨¡å‹
                ollama.pull(model_name)
                print(f"   âœ… {model_name} ä¸‹è½½æˆåŠŸï¼")
                return True
                
            except Exception as e:
                print(f"   âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}")
                continue
        
        print("   âŒ æ‰€æœ‰æ¨èæ¨¡å‹ä¸‹è½½éƒ½å¤±è´¥äº†")
        return False
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹å®‰è£…è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def suggest_solutions(issues):
    """æ ¹æ®å‘ç°çš„é—®é¢˜æä¾›è§£å†³æ–¹æ¡ˆ"""
    print("\n" + "="*60)
    print("ğŸ”§ è§£å†³æ–¹æ¡ˆå»ºè®®")
    print("="*60)
    
    if not issues:
        print("âœ… Ollamaé…ç½®æ­£å¸¸ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨åŒ»å­¦æŠ¥å‘ŠåŠŸèƒ½ï¼")
        return
    
    print("å‘ç°çš„é—®é¢˜:")
    for i, issue in enumerate(issues, 1):
        print(f"{i}. {issue}")
    
    print("\nğŸ’¡ è§£å†³æ­¥éª¤:")
    
    if "ollamaåŒ…" in str(issues):
        print("\n1. å®‰è£…ollamaåŒ…:")
        print("   pip install ollama")
    
    if "è¿›ç¨‹æœªè¿è¡Œ" in str(issues):
        print("\n2. å¯åŠ¨OllamaæœåŠ¡:")
        print("   ollama serve")
        print("   (åœ¨å¦ä¸€ä¸ªç»ˆç«¯çª—å£ä¸­è¿è¡Œï¼Œä¿æŒè¿è¡ŒçŠ¶æ€)")
    
    if "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹" in str(issues) or "æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹" in str(issues):
        print("\n3. ä¸‹è½½æ¨èçš„åŒ»å­¦æ¨¡å‹:")
        print("   æ‰‹åŠ¨ä¸‹è½½:")
        print("   ollama pull llama3.1:8b")
        print("   # æˆ–è€…")
        print("   ollama pull llama2:7b")
        
        # æä¾›è‡ªåŠ¨å®‰è£…é€‰é¡¹
        choice = input("\næ˜¯å¦ç°åœ¨è‡ªåŠ¨ä¸‹è½½æ¨èæ¨¡å‹ï¼Ÿè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ (y/n): ").lower().strip()
        if choice in ['y', 'yes', 'æ˜¯']:
            success = auto_install_recommended_model()
            if success:
                print("\nğŸ‰ æ¨¡å‹å®‰è£…æˆåŠŸï¼ç°åœ¨å¯ä»¥é‡å¯Webåº”ç”¨äº«å—å®Œæ•´çš„åŒ»å­¦æŠ¥å‘ŠåŠŸèƒ½äº†ã€‚")
            else:
                print("\nâŒ è‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œè¯·å°è¯•æ‰‹åŠ¨å®‰è£…ã€‚")
    
    if "è¿æ¥å¤±è´¥" in str(issues):
        print("\n4. æ£€æŸ¥ç½‘ç»œå’Œé˜²ç«å¢™:")
        print("   - ç¡®ä¿ç«¯å£11434æœªè¢«å ç”¨")
        print("   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
        print("   - å°è¯•é‡å¯OllamaæœåŠ¡")
        print("   - æ£€æŸ¥Ollamaç‰ˆæœ¬å…¼å®¹æ€§")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¦™ Ollamaè¿æ¥è¯Šæ–­å·¥å…·")
    print("="*60)
    
    issues = []
    
    # 1. æµ‹è¯•åŒ…å¯¼å…¥
    import_ok, ollama_module = test_ollama_import()
    if not import_ok:
        issues.append("ollamaåŒ…å¯¼å…¥å¤±è´¥")
        suggest_solutions(issues)
        return
    
    # 2. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
    process_running = check_ollama_process()
    if process_running is False:
        issues.append("Ollamaè¿›ç¨‹æœªè¿è¡Œ")
    
    # 3. æµ‹è¯•æœåŠ¡è¿æ¥
    service_ok, models = test_ollama_service()
    if not service_ok:
        issues.append("OllamaæœåŠ¡è¿æ¥å¤±è´¥")
    elif not models:
        issues.append("æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹")
    
    # 4. æµ‹è¯•ç”ŸæˆåŠŸèƒ½ï¼ˆä»…åœ¨æœ‰æ¨¡å‹æ—¶æµ‹è¯•ï¼‰
    if service_ok and models:
        generation_ok = test_model_generation()
        if not generation_ok:
            issues.append("æ¨¡å‹ç”ŸæˆåŠŸèƒ½å¼‚å¸¸")
    
    # æä¾›è§£å†³æ–¹æ¡ˆ
    suggest_solutions(issues)
    
    print("\n" + "="*60)
    if not issues:
        print("ğŸ‰ è¯Šæ–­å®Œæˆ: Ollamaé…ç½®æ­£å¸¸!")
        print("ç°åœ¨å¯ä»¥é‡å¯Webåº”ç”¨ä»¥ä½¿ç”¨å®Œæ•´çš„åŒ»å­¦æŠ¥å‘ŠåŠŸèƒ½ã€‚")
    else:
        print(f"ğŸ”§ è¯Šæ–­å®Œæˆ: å‘ç° {len(issues)} ä¸ªé—®é¢˜éœ€è¦ä¿®å¤")
    print("="*60)

if __name__ == "__main__":
    main() 