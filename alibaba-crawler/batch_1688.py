#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ‰¹é‡1688å•†å“ä¿¡æ¯æå–å™¨
æ”¯æŒä¸€æ¬¡æ€§å¤„ç†å¤šæ¡å•†å“é“¾æ¥
"""

import time
import random
import json
import csv
import os
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import requests

class Batch1688Crawler:
    def __init__(self):
        self.driver = None
        self.all_products_data = []
        self.setup_driver()
        self.setup_output_folders()
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def setup_output_folders(self):
        """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
        folders = ['images', 'data', 'logs', 'batch_results']
        for folder in folders:
            if not os.path.exists(folder):
                os.makedirs(folder)
                print(f"âœ… åˆ›å»ºæ–‡ä»¶å¤¹: {folder}")
    
    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            options = Options()
            options.headless = False
            
            # åæ£€æµ‹è®¾ç½®
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-extensions')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            
            # éšæœºç”¨æˆ·ä»£ç†
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            options.add_argument(f'--user-agent={random.choice(user_agents)}')
            
            try:
                service = Service(executable_path="geckodriver.exe")
                self.driver = webdriver.Firefox(service=service, options=options)
            except TypeError:
                self.driver = webdriver.Firefox(executable_path="geckodriver.exe", options=options)
            
            # éšè—webdriverå±æ€§
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    def wait_and_handle_captcha(self):
        """ç­‰å¾…å¹¶å¤„ç†éªŒè¯ç """
        captcha_keywords = ["éªŒè¯ç ", "captcha", "æ»‘åŠ¨éªŒè¯", "ç‚¹å‡»éªŒè¯", "æ‹–åŠ¨", "security"]
        
        for keyword in captcha_keywords:
            try:
                elements = self.driver.find_elements(By.XPATH, f"//*[contains(text(), '{keyword}')]")
                if elements:
                    print(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç : {keyword}")
                    print("ğŸ“‹ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼ŒéªŒè¯å®Œæˆå...")
                    input("æŒ‰å›è½¦ç»§ç»­...")
                    return True
            except:
                pass
        return False
    
    def human_simulate(self):
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # éšæœºæ»šåŠ¨
        for _ in range(random.randint(2, 4)):
            scroll_y = random.randint(100, 400)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_y});")
            time.sleep(random.uniform(0.3, 1.0))
        
        # åœé¡¿
        time.sleep(random.uniform(0.5, 2.0))
    
    def process_multiple_urls(self, urls):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
        total_urls = len(urls)
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {total_urls} ä¸ªå•†å“é“¾æ¥...")
        
        # é¦–æ¬¡è®¿é—®1688é¦–é¡µ
        print("ğŸ“ åˆå§‹åŒ–: è®¿é—®1688é¦–é¡µ...")
        self.driver.get("https://www.1688.com")
        time.sleep(random.uniform(3, 6))
        self.wait_and_handle_captcha()
        
        successful_count = 0
        failed_urls = []
        
        for index, url in enumerate(urls, 1):
            try:
                print(f"\n{'='*60}")
                print(f"ğŸ“Š è¿›åº¦: {index}/{total_urls} - å¤„ç†ç¬¬ {index} ä¸ªå•†å“")
                print(f"ğŸ”— URL: {url}")
                print('='*60)
                
                # å¤„ç†å•ä¸ªå•†å“
                product_data = self.extract_single_product(url, index)
                
                if product_data:
                    self.all_products_data.append(product_data)
                    successful_count += 1
                    print(f"âœ… ç¬¬ {index} ä¸ªå•†å“å¤„ç†æˆåŠŸ")
                    
                    # ä¿å­˜å•ä¸ªå•†å“æ•°æ®ï¼ˆå¤‡ä»½ï¼‰
                    self.save_single_product(product_data, index)
                else:
                    failed_urls.append((index, url))
                    print(f"âŒ ç¬¬ {index} ä¸ªå•†å“å¤„ç†å¤±è´¥")
                
                # éšæœºé—´éš”ï¼Œé¿å…è¢«æ£€æµ‹
                if index < total_urls:
                    delay = random.uniform(3, 8)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå•†å“...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {index} ä¸ªå•†å“æ—¶å‡ºé”™: {e}")
                failed_urls.append((index, url))
        
        # å¤„ç†ç»“æœæ±‡æ€»
        self.print_batch_summary(successful_count, total_urls, failed_urls)
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        if self.all_products_data:
            self.save_batch_results()
        
        return self.all_products_data
    
    def extract_single_product(self, url, index):
        """æå–å•ä¸ªå•†å“ä¿¡æ¯"""
        try:
            print(f"ğŸ” è®¿é—®å•†å“é¡µé¢...")
            self.driver.get(url)
            time.sleep(random.uniform(4, 7))
            
            # æ£€æŸ¥éªŒè¯ç 
            if self.wait_and_handle_captcha():
                time.sleep(2)
            
            # æ¨¡æ‹Ÿäººç±»æµè§ˆ
            self.human_simulate()
            
            # æå–ä¿¡æ¯
            product_info = self.extract_all_data(index)
            
            return product_info
            
        except Exception as e:
            print(f"âŒ æå–ç¬¬ {index} ä¸ªå•†å“å¤±è´¥: {e}")
            return None
    
    def extract_all_data(self, index):
        """æå–æ‰€æœ‰å¯èƒ½çš„æ•°æ®"""
        data = {
            'index': index,
            'url': self.driver.current_url,
            'timestamp': datetime.now().isoformat(),
            'title': self.extract_title(),
            'price': self.extract_price(),
            'images': self.extract_images(),
            'supplier': self.extract_supplier(),
            'specifications': self.extract_specifications(),
            'description': self.extract_description(),
            'moq': self.extract_moq(),
            'contact_info': self.extract_contact_info()
        }
        
        return data
    
    def extract_title(self):
        """æå–å•†å“æ ‡é¢˜"""
        selectors = [
            'h1', '.offer-title', '.d-title', '.detail-title',
            '[class*="title"]', '[class*="name"]', '.product-name',
            'title', '[data-spm-anchor-id*="title"]'
        ]
        
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 3:
                    print(f"âœ… æ ‡é¢˜: {text[:50]}...")
                    return text
            except:
                continue
        
        # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
        try:
            page_title = self.driver.title
            if page_title and "1688" not in page_title:
                print(f"âœ… é¡µé¢æ ‡é¢˜: {page_title}")
                return page_title
        except:
            pass
            
        print("âŒ æœªæ‰¾åˆ°å•†å“æ ‡é¢˜")
        return None
    
    def extract_price(self):
        """æå–ä»·æ ¼ä¿¡æ¯"""
        price_selectors = [
            '.price', '.offer-price', '.d-price', '.unit-price',
            '[class*="price"]', '[data-testid*="price"]',
            '.price-range', '.price-original', '.price-now'
        ]
        
        prices = []
        
        # å°è¯•CSSé€‰æ‹©å™¨
        for selector in price_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and any(char in text for char in ['ï¿¥', 'Â¥', 'å…ƒ', '.']):
                        prices.append(text)
            except:
                continue
        
        # æ­£åˆ™è¡¨è¾¾å¼æå–
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        price_patterns = [
            r'ï¿¥[\d,.]+', r'Â¥[\d,.]+', r'\d+\.\d+å…ƒ',
            r'\d+\.\d+-\d+\.\d+', r'\d+\.\d+èµ·'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, page_text)
            prices.extend(matches)
        
        if prices:
            unique_prices = list(set(prices))
            print(f"âœ… ä»·æ ¼: {unique_prices[:3]}")
            return unique_prices[:3]
        
        print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
        return None
    
    def extract_images(self):
        """æå–å•†å“å›¾ç‰‡"""
        images = []
        
        try:
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            for img in img_elements:
                try:
                    img_url = None
                    for attr in ['src', 'data-src', 'data-original', 'data-lazy']:
                        url = img.get_attribute(attr)
                        if url and url.startswith('http') and any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            img_url = url
                            break
                    
                    if img_url:
                        alt = img.get_attribute('alt') or ''
                        width = img.get_attribute('width') or 0
                        height = img.get_attribute('height') or 0
                        
                        images.append({
                            'url': img_url,
                            'alt': alt,
                            'width': width,
                            'height': height
                        })
                        
                        if len(images) >= 8:  # æœ€å¤š8å¼ å›¾ç‰‡
                            break
                            
                except:
                    continue
            
            if images:
                print(f"âœ… æå–åˆ° {len(images)} å¼ å›¾ç‰‡")
                return images
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡æå–å¤±è´¥: {e}")
        
        return []
    
    def extract_supplier(self):
        """æå–ä¾›åº”å•†ä¿¡æ¯"""
        supplier_selectors = [
            '.company-name', '.supplier-name', '.shop-name',
            '[class*="company"]', '[class*="supplier"]', '[class*="shop"]'
        ]
        
        for selector in supplier_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 2:
                    print(f"âœ… ä¾›åº”å•†: {text}")
                    return text
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°ä¾›åº”å•†ä¿¡æ¯")
        return None
    
    def extract_specifications(self):
        """æå–å•†å“è§„æ ¼"""
        specs = {}
        
        try:
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for table in tables:
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 2:
                        key = cells[0].text.strip()
                        value = cells[1].text.strip()
                        if key and value:
                            specs[key] = value
        except:
            pass
        
        if specs:
            print(f"âœ… è§„æ ¼å‚æ•°: {len(specs)} é¡¹")
            return specs
        
        print("âŒ æœªæ‰¾åˆ°è§„æ ¼å‚æ•°")
        return {}
    
    def extract_description(self):
        """æå–å•†å“æè¿°"""
        desc_selectors = [
            '.description', '.detail-desc', '.product-desc',
            '[class*="desc"]', '[class*="detail"]'
        ]
        
        for selector in desc_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 10:
                    print(f"âœ… æè¿°: {text[:50]}...")
                    return text
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°å•†å“æè¿°")
        return None
    
    def extract_moq(self):
        """æå–æœ€å°èµ·è®¢é‡"""
        moq_keywords = ["èµ·è®¢é‡", "æœ€å°", "MOQ", "èµ·æ‰¹"]
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        
        for keyword in moq_keywords:
            pattern = rf'{keyword}[ï¼š:]\s*(\d+)'
            match = re.search(pattern, page_text)
            if match:
                moq_value = match.group(1)
                print(f"âœ… èµ·è®¢é‡: {moq_value}")
                return moq_value
        
        print("âŒ æœªæ‰¾åˆ°èµ·è®¢é‡ä¿¡æ¯")
        return None
    
    def extract_contact_info(self):
        """æå–è”ç³»æ–¹å¼"""
        contact_info = {}
        
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        phone_pattern = r'1[3-9]\d{9}'
        phones = re.findall(phone_pattern, page_text)
        if phones:
            contact_info['phone'] = list(set(phones))[:3]
        
        if contact_info:
            print(f"âœ… è”ç³»ä¿¡æ¯: {len(contact_info)} é¡¹")
        return contact_info
    
    def save_single_product(self, product_data, index):
        """ä¿å­˜å•ä¸ªå•†å“æ•°æ®ï¼ˆå¤‡ä»½ï¼‰"""
        try:
            filename = f"data/product_{self.session_timestamp}_{index:03d}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(product_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜å•ä¸ªå•†å“æ•°æ®å¤±è´¥: {e}")
    
    def save_batch_results(self):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            # ä¿å­˜å®Œæ•´JSONæ•°æ®
            json_file = f"batch_results/batch_{self.session_timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_products_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ‰¹é‡JSONæ•°æ®å·²ä¿å­˜: {json_file}")
            
            # ä¿å­˜æ±‡æ€»CSV
            csv_file = f"batch_results/batch_summary_{self.session_timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['åºå·', 'URL', 'å•†å“æ ‡é¢˜', 'ä»·æ ¼', 'ä¾›åº”å•†', 'å›¾ç‰‡æ•°é‡', 'è§„æ ¼æ•°é‡'])
                
                for product in self.all_products_data:
                    writer.writerow([
                        product.get('index', ''),
                        product.get('url', ''),
                        product.get('title', ''),
                        str(product.get('price', [])[:2]) if product.get('price') else '',
                        product.get('supplier', ''),
                        len(product.get('images', [])),
                        len(product.get('specifications', {}))
                    ])
            print(f"âœ… æ‰¹é‡CSVæ±‡æ€»å·²ä¿å­˜: {csv_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ‰¹é‡ç»“æœå¤±è´¥: {e}")
    
    def print_batch_summary(self, successful_count, total_urls, failed_urls):
        """æ‰“å°æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»"""
        print(f"\n" + "="*80)
        print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»")
        print("="*80)
        print(f"æ€»é“¾æ¥æ•°: {total_urls}")
        print(f"æˆåŠŸå¤„ç†: {successful_count}")
        print(f"å¤„ç†å¤±è´¥: {len(failed_urls)}")
        print(f"æˆåŠŸç‡: {(successful_count/total_urls)*100:.1f}%")
        
        if failed_urls:
            print(f"\nâŒ å¤±è´¥çš„é“¾æ¥:")
            for index, url in failed_urls:
                print(f"  {index}. {url}")
        
        print("="*80)
    
    def download_all_images(self):
        """ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡"""
        if not self.all_products_data:
            return
        
        print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡...")
        
        for product in self.all_products_data:
            if product.get('images'):
                index = product.get('index', 0)
                self.download_product_images(product['images'], index)
    
    def download_product_images(self, images_data, product_index):
        """ä¸‹è½½å•ä¸ªå•†å“çš„å›¾ç‰‡"""
        for i, img_data in enumerate(images_data[:3]):  # æ¯ä¸ªå•†å“æœ€å¤šä¸‹è½½3å¼ 
            try:
                img_url = img_data['url']
                response = requests.get(img_url, timeout=10)
                
                if response.status_code == 200:
                    ext = img_url.split('.')[-1].split('?')[0]
                    if ext not in ['jpg', 'jpeg', 'png', 'webp']:
                        ext = 'jpg'
                    
                    filename = f"images/batch_{self.session_timestamp}_product_{product_index:03d}_{i+1}.{ext}"
                    with open(filename, 'wb') as f:
                        f.write(response.content)
                    print(f"âœ… å›¾ç‰‡å·²ä¸‹è½½: {filename}")
                    
            except Exception as e:
                print(f"âŒ å•†å“ {product_index} å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            input("\nğŸ“‹ æ‰¹é‡å¤„ç†å®Œæˆï¼æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

def get_urls_from_user():
    """ä»ç”¨æˆ·è¾“å…¥è·å–URLåˆ—è¡¨"""
    print("ğŸ”— è¯·è¾“å…¥å•†å“é“¾æ¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥å®ŒæˆåæŒ‰å›è½¦+Ctrl+Dç»“æŸï¼‰:")
    print("æˆ–è€…ç›´æ¥ç²˜è´´å¤šè¡Œé“¾æ¥ï¼š")
    print("-" * 60)
    
    urls = []
    try:
        while True:
            line = input().strip()
            if line:
                if line.startswith('http') and '1688.com' in line:
                    urls.append(line)
                    print(f"âœ… å·²æ·»åŠ ç¬¬ {len(urls)} ä¸ªé“¾æ¥")
                else:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„1688å•†å“é“¾æ¥")
    except EOFError:
        pass
    
    return urls

def get_urls_from_file():
    """ä»input.txtæ–‡ä»¶è¯»å–URLåˆ—è¡¨"""
    filename = "input.txt"
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    if not os.path.exists(filename):
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("https://detail.1688.com/offer/775610063728.html?offerId=775610063728&spm=a260k.home2025.recommendpart.18\n")
            f.write("# è¯·åœ¨ä¸Šæ–¹æ·»åŠ æ›´å¤š1688å•†å“é“¾æ¥ï¼Œæ¯è¡Œä¸€ä¸ª\n")
            f.write("# ä»¥ # å¼€å¤´çš„è¡Œä¼šè¢«å¿½ç•¥\n")
        print(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {filename}")
        print("è¯·ç¼–è¾‘è¯¥æ–‡ä»¶æ·»åŠ æ›´å¤šé“¾æ¥ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åº")
        return []
    
    urls = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    if 'detail.1688.com' in line:
                        urls.append(line)
                        print(f"âœ… ç¬¬ {line_num} è¡Œ: å·²æ·»åŠ é“¾æ¥")
                    else:
                        print(f"âŒ ç¬¬ {line_num} è¡Œä¸æ˜¯æœ‰æ•ˆçš„1688é“¾æ¥: {line[:50]}...")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
    
    return urls

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‰¹é‡1688å•†å“ä¿¡æ¯æå–å™¨")
    print("="*50)
    
    # é€‰æ‹©è¾“å…¥æ–¹å¼
    print("è¯·é€‰æ‹©é“¾æ¥è¾“å…¥æ–¹å¼:")
    print("1. æ‰‹åŠ¨è¾“å…¥é“¾æ¥")
    print("2. ä»æ–‡ä»¶è¯»å–é“¾æ¥ (input.txt)")
    
    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        if choice in ['1', '2']:
            break
        print("âŒ è¯·è¾“å…¥ 1 æˆ– 2")
    
    # è·å–URLåˆ—è¡¨
    if choice == '1':
        urls = get_urls_from_user()
    else:
        urls = get_urls_from_file()
    
    if not urls:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é“¾æ¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"\nâœ… å…±æ‰¾åˆ° {len(urls)} ä¸ªæœ‰æ•ˆé“¾æ¥")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url[:60]}...")
    
    # ç¡®è®¤å¼€å§‹å¤„ç†
    confirm = input(f"\næ˜¯å¦å¼€å§‹å¤„ç†è¿™ {len(urls)} ä¸ªé“¾æ¥ï¼Ÿ(y/n): ").strip().lower()
    if confirm != 'y':
        print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return
    
    # å¼€å§‹æ‰¹é‡å¤„ç†
    crawler = None
    try:
        crawler = Batch1688Crawler()
        results = crawler.process_multiple_urls(urls)
        
        if results:
            # ä¸‹è½½å›¾ç‰‡
            download_images = input("\næ˜¯å¦ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡ï¼Ÿ(y/n): ").strip().lower()
            if download_images == 'y':
                crawler.download_all_images()
            
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
            print(f"  - batch_results/ æ–‡ä»¶å¤¹: æ‰¹é‡å¤„ç†ç»“æœ")
            print(f"  - data/ æ–‡ä»¶å¤¹: å•ä¸ªå•†å“JSONæ–‡ä»¶")
            print(f"  - images/ æ–‡ä»¶å¤¹: å•†å“å›¾ç‰‡")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å•†å“")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    
    finally:
        if crawler:
            crawler.close()

if __name__ == "__main__":
    main()
