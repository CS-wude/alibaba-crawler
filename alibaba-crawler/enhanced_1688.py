#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰ˆ1688å•†å“ä¿¡æ¯æå–å™¨
åŒ…å«å®Œæ•´çš„å•†å“ä¿¡æ¯æå–å’Œä¿å­˜åŠŸèƒ½
"""

import time
import random
import json
import csv
import os
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import requests

class Enhanced1688Crawler:
    def __init__(self):
        self.driver = None
        self.product_data = {}
        self.setup_driver()
        self.setup_output_folders()
    
    def setup_output_folders(self):
        """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
        folders = ['images', 'data', 'logs']
        for folder in folders:
            if not os.path.exists(folder):
                os.makedirs(folder)
                print(f"âœ… åˆ›å»ºæ–‡ä»¶å¤¹: {folder}")
    
    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            options = Options()
            options.headless = False
            
            # åæ£€æµ‹è®¾ç½®
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-extensions')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            
            # éšæœºç”¨æˆ·ä»£ç†
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            options.add_argument(f'--user-agent={random.choice(user_agents)}')
            
            try:
                service = Service(executable_path="geckodriver.exe")
                self.driver = webdriver.Firefox(service=service, options=options)
            except TypeError:
                self.driver = webdriver.Firefox(executable_path="geckodriver.exe", options=options)
            
            # éšè—webdriverå±æ€§
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    def wait_and_handle_captcha(self):
        """ç­‰å¾…å¹¶å¤„ç†éªŒè¯ç """
        captcha_keywords = ["éªŒè¯ç ", "captcha", "æ»‘åŠ¨éªŒè¯", "ç‚¹å‡»éªŒè¯", "æ‹–åŠ¨", "security"]
        
        for keyword in captcha_keywords:
            try:
                elements = self.driver.find_elements(By.XPATH, f"//*[contains(text(), '{keyword}')]")
                if elements:
                    print(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç : {keyword}")
                    print("ğŸ“‹ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼ŒéªŒè¯å®Œæˆå...")
                    input("æŒ‰å›è½¦ç»§ç»­...")
                    return True
            except:
                pass
        return False
    
    def human_simulate(self):
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # éšæœºæ»šåŠ¨
        for _ in range(random.randint(2, 5)):
            scroll_y = random.randint(100, 500)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_y});")
            time.sleep(random.uniform(0.5, 1.5))
        
        # åœé¡¿
        time.sleep(random.uniform(1, 3))
    
    def extract_comprehensive_info(self, url):
        """æå–å®Œæ•´å•†å“ä¿¡æ¯"""
        try:
            print(f"ğŸ” å¼€å§‹æå–å•†å“ä¿¡æ¯: {url}")
            
            # åˆ†æ­¥è®¿é—®
            print("ğŸ“ æ­¥éª¤1: è®¿é—®1688é¦–é¡µ...")
            self.driver.get("https://www.1688.com")
            time.sleep(random.uniform(3, 6))
            
            # æ£€æŸ¥éªŒè¯ç 
            self.wait_and_handle_captcha()
            
            print("ğŸ“ æ­¥éª¤2: è®¿é—®å•†å“é¡µé¢...")
            self.driver.get(url)
            time.sleep(random.uniform(5, 8))
            
            # å†æ¬¡æ£€æŸ¥éªŒè¯ç 
            if self.wait_and_handle_captcha():
                time.sleep(2)
            
            # æ¨¡æ‹Ÿäººç±»æµè§ˆ
            print("ğŸ“ æ­¥éª¤3: æ¨¡æ‹Ÿæµè§ˆè¡Œä¸º...")
            self.human_simulate()
            
            # æå–ä¿¡æ¯
            print("ğŸ“ æ­¥éª¤4: æå–å•†å“ä¿¡æ¯...")
            product_info = self.extract_all_data()
            
            # ä¿å­˜é¡µé¢æºç 
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            with open(f"logs/page_source_{timestamp}.html", "w", encoding="utf-8") as f:
                f.write(self.driver.page_source)
            
            return product_info
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            return None
    
    def extract_all_data(self):
        """æå–æ‰€æœ‰å¯èƒ½çš„æ•°æ®"""
        data = {
            'url': self.driver.current_url,
            'timestamp': datetime.now().isoformat(),
            'title': self.extract_title(),
            'price': self.extract_price(),
            'images': self.extract_images(),
            'supplier': self.extract_supplier(),
            'specifications': self.extract_specifications(),
            'description': self.extract_description(),
            'moq': self.extract_moq(),
            'contact_info': self.extract_contact_info()
        }
        
        return data
    
    def extract_title(self):
        """æå–å•†å“æ ‡é¢˜"""
        selectors = [
            'h1', '.offer-title', '.d-title', '.detail-title',
            '[class*="title"]', '[class*="name"]', '.product-name',
            'title', '[data-spm-anchor-id*="title"]'
        ]
        
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 3:
                    print(f"âœ… æ ‡é¢˜: {text[:50]}...")
                    return text
            except:
                continue
        
        # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
        try:
            page_title = self.driver.title
            if page_title and "1688" not in page_title:
                print(f"âœ… é¡µé¢æ ‡é¢˜: {page_title}")
                return page_title
        except:
            pass
            
        print("âŒ æœªæ‰¾åˆ°å•†å“æ ‡é¢˜")
        return None
    
    def extract_price(self):
        """æå–ä»·æ ¼ä¿¡æ¯"""
        # ä»·æ ¼ç›¸å…³çš„CSSé€‰æ‹©å™¨
        price_selectors = [
            '.price', '.offer-price', '.d-price', '.unit-price',
            '[class*="price"]', '[data-testid*="price"]',
            '.price-range', '.price-original', '.price-now'
        ]
        
        prices = []
        
        # å°è¯•CSSé€‰æ‹©å™¨
        for selector in price_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and any(char in text for char in ['ï¿¥', 'Â¥', 'å…ƒ', '.']):
                        prices.append(text)
            except:
                continue
        
        # æ­£åˆ™è¡¨è¾¾å¼æå–
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        price_patterns = [
            r'ï¿¥[\d,.]+', r'Â¥[\d,.]+', r'\d+\.\d+å…ƒ',
            r'\d+\.\d+-\d+\.\d+', r'\d+\.\d+èµ·'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, page_text)
            prices.extend(matches)
        
        if prices:
            # å»é‡å¹¶è¿”å›æœ€ç›¸å…³çš„ä»·æ ¼
            unique_prices = list(set(prices))
            print(f"âœ… ä»·æ ¼: {unique_prices[:3]}")
            return unique_prices[:3]
        
        print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
        return None
    
    def extract_images(self):
        """æå–å•†å“å›¾ç‰‡"""
        images = []
        
        try:
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            print(f"ğŸ“Š æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ")
            
            for img in img_elements:
                try:
                    # å°è¯•ä¸åŒçš„å›¾ç‰‡URLå±æ€§
                    img_url = None
                    for attr in ['src', 'data-src', 'data-original', 'data-lazy']:
                        url = img.get_attribute(attr)
                        if url and url.startswith('http') and any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            img_url = url
                            break
                    
                    if img_url:
                        alt = img.get_attribute('alt') or ''
                        width = img.get_attribute('width') or 0
                        height = img.get_attribute('height') or 0
                        
                        images.append({
                            'url': img_url,
                            'alt': alt,
                            'width': width,
                            'height': height
                        })
                        
                        if len(images) >= 10:  # æœ€å¤š10å¼ å›¾ç‰‡
                            break
                            
                except:
                    continue
            
            if images:
                print(f"âœ… æå–åˆ° {len(images)} å¼ å›¾ç‰‡")
                return images
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡æå–å¤±è´¥: {e}")
        
        return []
    
    def extract_supplier(self):
        """æå–ä¾›åº”å•†ä¿¡æ¯"""
        supplier_selectors = [
            '.company-name', '.supplier-name', '.shop-name',
            '[class*="company"]', '[class*="supplier"]', '[class*="shop"]'
        ]
        
        for selector in supplier_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 2:
                    print(f"âœ… ä¾›åº”å•†: {text}")
                    return text
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°ä¾›åº”å•†ä¿¡æ¯")
        return None
    
    def extract_specifications(self):
        """æå–å•†å“è§„æ ¼"""
        specs = {}
        
        # å°è¯•è¡¨æ ¼å½¢å¼çš„è§„æ ¼
        try:
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for table in tables:
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 2:
                        key = cells[0].text.strip()
                        value = cells[1].text.strip()
                        if key and value:
                            specs[key] = value
        except:
            pass
        
        if specs:
            print(f"âœ… è§„æ ¼å‚æ•°: {len(specs)} é¡¹")
            return specs
        
        print("âŒ æœªæ‰¾åˆ°è§„æ ¼å‚æ•°")
        return {}
    
    def extract_description(self):
        """æå–å•†å“æè¿°"""
        desc_selectors = [
            '.description', '.detail-desc', '.product-desc',
            '[class*="desc"]', '[class*="detail"]'
        ]
        
        for selector in desc_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 10:
                    print(f"âœ… æè¿°: {text[:50]}...")
                    return text
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°å•†å“æè¿°")
        return None
    
    def extract_moq(self):
        """æå–æœ€å°èµ·è®¢é‡"""
        moq_keywords = ["èµ·è®¢é‡", "æœ€å°", "MOQ", "èµ·æ‰¹"]
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        
        for keyword in moq_keywords:
            pattern = rf'{keyword}[ï¼š:]\s*(\d+)'
            match = re.search(pattern, page_text)
            if match:
                moq_value = match.group(1)
                print(f"âœ… èµ·è®¢é‡: {moq_value}")
                return moq_value
        
        print("âŒ æœªæ‰¾åˆ°èµ·è®¢é‡ä¿¡æ¯")
        return None
    
    def extract_contact_info(self):
        """æå–è”ç³»æ–¹å¼"""
        contact_info = {}
        
        # æŸ¥æ‰¾ç”µè¯å·ç 
        page_text = self.driver.find_element(By.TAG_NAME, "body").text
        phone_pattern = r'1[3-9]\d{9}'
        phones = re.findall(phone_pattern, page_text)
        if phones:
            contact_info['phone'] = list(set(phones))[:3]
        
        print(f"âœ… è”ç³»ä¿¡æ¯: {len(contact_info)} é¡¹")
        return contact_info
    
    def save_data(self, product_data, format_type='all'):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format_type in ['json', 'all']:
            # ä¿å­˜ä¸ºJSON
            json_file = f"data/product_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(product_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {json_file}")
        
        if format_type in ['csv', 'all']:
            # ä¿å­˜ä¸ºCSV
            csv_file = f"data/product_{timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['å­—æ®µ', 'å€¼'])
                for key, value in product_data.items():
                    if isinstance(value, (list, dict)):
                        value = str(value)
                    writer.writerow([key, value])
            print(f"âœ… CSVæ•°æ®å·²ä¿å­˜: {csv_file}")
    
    def download_images(self, images_data):
        """ä¸‹è½½å•†å“å›¾ç‰‡"""
        if not images_data:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for i, img_data in enumerate(images_data[:5]):  # æœ€å¤šä¸‹è½½5å¼ 
            try:
                img_url = img_data['url']
                response = requests.get(img_url, timeout=10)
                
                if response.status_code == 200:
                    # è·å–æ–‡ä»¶æ‰©å±•å
                    ext = img_url.split('.')[-1].split('?')[0]
                    if ext not in ['jpg', 'jpeg', 'png', 'webp']:
                        ext = 'jpg'
                    
                    filename = f"images/product_{timestamp}_{i+1}.{ext}"
                    with open(filename, 'wb') as f:
                        f.write(response.content)
                    print(f"âœ… å›¾ç‰‡å·²ä¸‹è½½: {filename}")
                    
            except Exception as e:
                print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i+1}: {e}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            input("\nğŸ“‹ æ•°æ®æå–å®Œæˆï¼æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    #url = "https://detail.1688.com/offer/775610063728.html?offerId=775610063728&spm=a260k.home2025.recommendpart.18"
    #url = "https://detail.1688.com/offer/963863008803.html?spm=a26352.13672862.offerlist.9.6b095d62nOjRkt"
    url = "https://detail.1688.com/offer/816228014618.html?topicCode=202508210010000000000001696268&optName=%E7%83%AD%E7%82%B9%E5%95%86%E6%9C%BA&topicName=%E6%89%8B%E8%A1%A8%E5%AE%9D%E8%97%8F%E9%9B%86&item_id=816228014618&offerId=816228014618&object_id=816228014618&spm=a260k.29939364.recommend.0"
    
    crawler = None
    try:
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆ1688å•†å“ä¿¡æ¯æå–å™¨...")
        
        crawler = Enhanced1688Crawler()
        product_data = crawler.extract_comprehensive_info(url)
        
        if product_data:
            print("\n" + "="*60)
            print("ğŸ“Š æå–ç»“æœæ±‡æ€»:")
            print("="*60)
            
            for key, value in product_data.items():
                if isinstance(value, list) and value:
                    print(f"{key}: {len(value)} é¡¹")
                elif isinstance(value, dict) and value:
                    print(f"{key}: {len(value)} é¡¹")
                elif value:
                    display_value = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
                    print(f"{key}: {display_value}")
                else:
                    print(f"{key}: æœªè·å–åˆ°")
            
            print("="*60)
            
            # ä¿å­˜æ•°æ®
            crawler.save_data(product_data)
            
            # ä¸‹è½½å›¾ç‰‡
            if product_data.get('images'):
                print("\nğŸ“¸ å¼€å§‹ä¸‹è½½å•†å“å›¾ç‰‡...")
                crawler.download_images(product_data['images'])
            
            print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
            print("ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
            print("  - data/ æ–‡ä»¶å¤¹: JSONå’ŒCSVæ•°æ®æ–‡ä»¶")
            print("  - images/ æ–‡ä»¶å¤¹: ä¸‹è½½çš„å•†å“å›¾ç‰‡")
            print("  - logs/ æ–‡ä»¶å¤¹: é¡µé¢æºç å¤‡ä»½")
            
        else:
            print("âŒ æœªèƒ½æå–åˆ°å•†å“ä¿¡æ¯")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    
    finally:
        if crawler:
            crawler.close()

if __name__ == "__main__":
    main()
