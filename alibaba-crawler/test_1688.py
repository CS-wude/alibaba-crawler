#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
1688å•†å“ä¿¡æ¯æå–æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•ä»1688ç½‘ç«™æå–å•†å“ä¿¡æ¯çš„å¯è¡Œæ€§
"""

import time
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import requests
import os

class Product1688Crawler:
    def __init__(self):
        self.driver = None
        self.setup_driver()
    
    def setup_driver(self):
        """è®¾ç½®Firefoxæµè§ˆå™¨"""
        try:
            options = Options()
            options.headless = False  # è®¾ä¸ºFalseä»¥ä¾¿è§‚å¯Ÿè¿‡ç¨‹
            
            # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„Selenium
            try:
                service = Service(executable_path="geckodriver.exe")
                self.driver = webdriver.Firefox(service=service, options=options)
            except TypeError:
                self.driver = webdriver.Firefox(executable_path="geckodriver.exe", options=options)
            
            print("âœ… Firefoxæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    def extract_product_info(self, url):
        """æå–å•†å“ä¿¡æ¯"""
        try:
            print(f"ğŸ” å¼€å§‹è®¿é—®: {url}")
            self.driver.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            product_info = {}
            
            # å°è¯•æå–å•†å“æ ‡é¢˜
            title_selectors = [
                'h1.d-title',
                '.d-title',
                'h1',
                '.offer-title',
                '.product-title'
            ]
            product_info['title'] = self.extract_text_by_selectors(title_selectors, "å•†å“æ ‡é¢˜")
            
            # å°è¯•æå–ä»·æ ¼ä¿¡æ¯
            price_selectors = [
                '.price-range',
                '.price-original',
                '.price-now',
                '.price',
                '[data-testid="price"]'
            ]
            product_info['price'] = self.extract_text_by_selectors(price_selectors, "ä»·æ ¼")
            
            # å°è¯•æå–å•†å“å›¾ç‰‡
            image_selectors = [
                '.detail-gallery img',
                '.offer-img img',
                '.product-img img',
                'img[data-src]'
            ]
            product_info['images'] = self.extract_images(image_selectors)
            
            # å°è¯•æå–ä¾›åº”å•†ä¿¡æ¯
            supplier_selectors = [
                '.company-name',
                '.supplier-name',
                '.shop-name'
            ]
            product_info['supplier'] = self.extract_text_by_selectors(supplier_selectors, "ä¾›åº”å•†")
            
            # å°è¯•æå–èµ·è®¢é‡
            moq_selectors = [
                '.amount-range',
                '.min-order',
                '.moq'
            ]
            product_info['moq'] = self.extract_text_by_selectors(moq_selectors, "èµ·è®¢é‡")
            
            return product_info
            
        except Exception as e:
            print(f"âŒ æå–å•†å“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def extract_text_by_selectors(self, selectors, info_type):
        """é€šè¿‡å¤šä¸ªé€‰æ‹©å™¨å°è¯•æå–æ–‡æœ¬"""
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text:
                    print(f"âœ… æˆåŠŸæå–{info_type}: {text[:50]}...")
                    return text
            except NoSuchElementException:
                continue
        
        print(f"âŒ æ— æ³•æå–{info_type}")
        return None
    
    def extract_images(self, selectors):
        """æå–å•†å“å›¾ç‰‡URL"""
        images = []
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    # å°è¯•ä¸åŒçš„å›¾ç‰‡URLå±æ€§
                    for attr in ['src', 'data-src', 'data-original']:
                        img_url = element.get_attribute(attr)
                        if img_url and img_url.startswith('http'):
                            images.append(img_url)
                            break
                if images:
                    print(f"âœ… æˆåŠŸæå–å›¾ç‰‡æ•°é‡: {len(images)}")
                    return images[:5]  # æœ€å¤šè¿”å›5å¼ å›¾ç‰‡
            except NoSuchElementException:
                continue
        
        print("âŒ æ— æ³•æå–å›¾ç‰‡")
        return []
    
    def save_to_csv(self, product_info, filename="1688_result.csv"):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("æ ‡é¢˜,ä»·æ ¼,ä¾›åº”å•†,èµ·è®¢é‡,å›¾ç‰‡æ•°é‡,å›¾ç‰‡URLs\n")
                
                title = product_info.get('title', 'æ— ')
                price = product_info.get('price', 'æ— ')
                supplier = product_info.get('supplier', 'æ— ')
                moq = product_info.get('moq', 'æ— ')
                images = product_info.get('images', [])
                image_count = len(images)
                image_urls = ';'.join(images)
                
                f.write(f'"{title}","{price}","{supplier}","{moq}",{image_count},"{image_urls}"\n')
            
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    # 1688å•†å“URL
    url = "https://detail.1688.com/offer/775610063728.html?offerId=775610063728&spm=a260k.home2025.recommendpart.18"
    
    crawler = None
    try:
        print("ğŸš€ å¼€å§‹1688å•†å“ä¿¡æ¯æå–æµ‹è¯•...")
        
        crawler = Product1688Crawler()
        product_info = crawler.extract_product_info(url)
        
        if product_info:
            print("\nğŸ“Š æå–ç»“æœ:")
            print("="*50)
            for key, value in product_info.items():
                if isinstance(value, list):
                    print(f"{key}: {len(value)} é¡¹")
                else:
                    print(f"{key}: {value}")
            print("="*50)
            
            # ä¿å­˜ç»“æœ
            crawler.save_to_csv(product_info)
            print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        else:
            print("âŒ æœªèƒ½æå–åˆ°å•†å“ä¿¡æ¯")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    
    finally:
        if crawler:
            crawler.close()

if __name__ == "__main__":
    main()