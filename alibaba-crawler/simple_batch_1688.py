#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç®€åŒ–ç‰ˆæ‰¹é‡1688å•†å“ä¿¡æ¯æå–å™¨
ç›´æ¥è¯»å–input.txtæ–‡ä»¶ä¸­çš„é“¾æ¥ï¼Œä¸€è¡Œä¸€ä¸ª
"""

import time
import random
import json
import csv
import os
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
import requests

class SimpleBatch1688:
    def __init__(self):
        self.driver = None
        self.all_products_data = []
        self.session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.setup_output_folders()
        self.setup_driver()
    
    def setup_output_folders(self):
        """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
        folders = ['images', 'data', 'logs', 'batch_results']
        for folder in folders:
            if not os.path.exists(folder):
                os.makedirs(folder)
    
    def setup_driver(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            options = Options()
            options.headless = False
            
            # åæ£€æµ‹è®¾ç½®
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-extensions')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--window-size=1920,1080')
            
            # éšæœºç”¨æˆ·ä»£ç†
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
            ]
            options.add_argument(f'--user-agent={random.choice(user_agents)}')
            
            try:
                service = Service(executable_path="geckodriver.exe")
                self.driver = webdriver.Firefox(service=service, options=options)
            except TypeError:
                self.driver = webdriver.Firefox(executable_path="geckodriver.exe", options=options)
            
            # éšè—webdriverå±æ€§
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    def read_urls_from_input(self):
        """ä»input.txtæ–‡ä»¶è¯»å–é“¾æ¥"""
        filename = "input.txt"
        
        if not os.path.exists(filename):
            print(f"âŒ æ‰¾ä¸åˆ° {filename} æ–‡ä»¶")
            print("è¯·åˆ›å»º input.txt æ–‡ä»¶ï¼Œå¹¶åœ¨å…¶ä¸­æ·»åŠ 1688å•†å“é“¾æ¥ï¼Œæ¯è¡Œä¸€ä¸ª")
            return []
        
        urls = []
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if 'detail.1688.com' in line:
                            urls.append(line)
                            print(f"âœ… ç¬¬ {line_num} è¡Œ: å·²è¯»å–é“¾æ¥")
                        elif line:  # ä¸ä¸ºç©ºä½†ä¸æ˜¯1688é“¾æ¥
                            print(f"âŒ ç¬¬ {line_num} è¡Œä¸æ˜¯æœ‰æ•ˆçš„1688é“¾æ¥: {line[:50]}...")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        
        return urls
    
    def wait_and_handle_captcha(self):
        """ç­‰å¾…å¹¶å¤„ç†éªŒè¯ç """
        captcha_keywords = ["éªŒè¯ç ", "captcha", "æ»‘åŠ¨éªŒè¯", "ç‚¹å‡»éªŒè¯", "æ‹–åŠ¨"]
        
        for keyword in captcha_keywords:
            try:
                elements = self.driver.find_elements(By.XPATH, f"//*[contains(text(), '{keyword}')]")
                if elements:
                    print(f"ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç : {keyword}")
                    print("ğŸ“‹ è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼ŒéªŒè¯å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
                    input()
                    return True
            except:
                pass
        return False
    
    def human_simulate(self):
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # éšæœºæ»šåŠ¨
        for _ in range(random.randint(2, 4)):
            scroll_y = random.randint(100, 400)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_y});")
            time.sleep(random.uniform(0.3, 1.0))
        time.sleep(random.uniform(0.5, 2.0))
    
    def process_all_urls(self, urls):
        """å¤„ç†æ‰€æœ‰URL"""
        total_urls = len(urls)
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {total_urls} ä¸ªå•†å“é“¾æ¥...")
        
        # é¦–æ¬¡è®¿é—®1688é¦–é¡µ
        print("ğŸ“ åˆå§‹åŒ–: è®¿é—®1688é¦–é¡µ...")
        self.driver.get("https://www.1688.com")
        time.sleep(random.uniform(3, 6))
        self.wait_and_handle_captcha()
        
        successful_count = 0
        failed_urls = []
        
        for index, url in enumerate(urls, 1):
            try:
                print(f"\n{'='*60}")
                print(f"ğŸ“Š è¿›åº¦: {index}/{total_urls} - å¤„ç†ç¬¬ {index} ä¸ªå•†å“")
                print(f"ğŸ”— {url[:80]}...")
                print('='*60)
                
                # å¤„ç†å•ä¸ªå•†å“
                product_data = self.extract_single_product(url, index)
                
                if product_data:
                    self.all_products_data.append(product_data)
                    successful_count += 1
                    print(f"âœ… ç¬¬ {index} ä¸ªå•†å“å¤„ç†æˆåŠŸ")
                    
                    # ä¿å­˜å•ä¸ªå•†å“æ•°æ®
                    self.save_single_product(product_data, index)
                else:
                    failed_urls.append((index, url))
                    print(f"âŒ ç¬¬ {index} ä¸ªå•†å“å¤„ç†å¤±è´¥")
                
                # éšæœºé—´éš”
                if index < total_urls:
                    delay = random.uniform(3, 8)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªå•†å“...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {index} ä¸ªå•†å“æ—¶å‡ºé”™: {e}")
                failed_urls.append((index, url))
        
        # å¤„ç†ç»“æœæ±‡æ€»
        self.print_summary(successful_count, total_urls, failed_urls)
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        if self.all_products_data:
            self.save_batch_results()
        
        return self.all_products_data
    
    def extract_single_product(self, url, index):
        """æå–å•ä¸ªå•†å“ä¿¡æ¯"""
        try:
            print(f"ğŸ” è®¿é—®å•†å“é¡µé¢...")
            self.driver.get(url)
            time.sleep(random.uniform(4, 7))
            
            # æ£€æŸ¥éªŒè¯ç 
            if self.wait_and_handle_captcha():
                time.sleep(2)
            
            # æ¨¡æ‹Ÿäººç±»æµè§ˆ
            self.human_simulate()
            
            # æå–ä¿¡æ¯
            product_info = {
                'index': index,
                'url': self.driver.current_url,
                'timestamp': datetime.now().isoformat(),
                'title': self.extract_title(),
                'price': self.extract_price(),
                'images': self.extract_images(),
                'supplier': self.extract_supplier(),
                'specifications': self.extract_specifications(),
                'moq': self.extract_moq()
            }
            
            return product_info
            
        except Exception as e:
            print(f"âŒ æå–ç¬¬ {index} ä¸ªå•†å“å¤±è´¥: {e}")
            return None
    
    def extract_title(self):
        """æå–å•†å“æ ‡é¢˜"""
        selectors = [
            'h1', '.offer-title', '.d-title', '.detail-title',
            '[class*="title"]', '[class*="name"]', '.product-name'
        ]
        
        for selector in selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 3:
                    print(f"âœ… æ ‡é¢˜: {text[:50]}...")
                    return text
            except:
                continue
        
        # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
        try:
            page_title = self.driver.title
            if page_title and "1688" not in page_title:
                print(f"âœ… é¡µé¢æ ‡é¢˜: {page_title}")
                return page_title
        except:
            pass
            
        print("âŒ æœªæ‰¾åˆ°å•†å“æ ‡é¢˜")
        return None
    
    def extract_price(self):
        """æå–ä»·æ ¼ä¿¡æ¯"""
        prices = []
        
        # CSSé€‰æ‹©å™¨æå–
        price_selectors = [
            '.price', '.offer-price', '.d-price', '.unit-price',
            '[class*="price"]', '.price-range', '.price-original', '.price-now'
        ]
        
        for selector in price_selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                for element in elements:
                    text = element.text.strip()
                    if text and any(char in text for char in ['ï¿¥', 'Â¥', 'å…ƒ', '.']):
                        prices.append(text)
            except:
                continue
        
        # æ­£åˆ™è¡¨è¾¾å¼æå–
        try:
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            price_patterns = [
                r'ï¿¥[\d,.]+', r'Â¥[\d,.]+', r'\d+\.\d+å…ƒ',
                r'\d+\.\d+-\d+\.\d+', r'\d+\.\d+èµ·'
            ]
            
            for pattern in price_patterns:
                matches = re.findall(pattern, page_text)
                prices.extend(matches)
        except:
            pass
        
        if prices:
            unique_prices = list(set(prices))
            print(f"âœ… ä»·æ ¼: {unique_prices[:3]}")
            return unique_prices[:3]
        
        print("âŒ æœªæ‰¾åˆ°ä»·æ ¼ä¿¡æ¯")
        return None
    
    def extract_images(self):
        """æå–å•†å“çš„å…¨éƒ¨å›¾ç‰‡"""
        images = []
        seen_urls = set()  # ç”¨äºå»é‡
        
        try:
            print("ğŸ” å¼€å§‹æå–å•†å“å›¾ç‰‡...")
            
            # 1. æå–æ‰€æœ‰imgæ ‡ç­¾çš„å›¾ç‰‡
            img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            print(f"ğŸ“Š æ‰¾åˆ° {len(img_elements)} ä¸ªimgå…ƒç´ ")
            
            for img in img_elements:
                try:
                    img_url = None
                    # å°è¯•å¤šç§å›¾ç‰‡URLå±æ€§
                    for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-img', 'data-url']:
                        url = img.get_attribute(attr)
                        if url and url.startswith('http'):
                            # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡URL
                            if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']):
                                img_url = url
                                break
                            # æ£€æŸ¥é˜¿é‡Œäº‘å›¾ç‰‡æœåŠ¡çš„URLæ¨¡å¼
                            elif 'alicdn.com' in url or 'img.alicdn.com' in url:
                                img_url = url
                                break
                    
                    if img_url and img_url not in seen_urls:
                        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å›¾æ ‡å’Œè£…é¥°å›¾ç‰‡
                        if self.is_product_image(img_url, img):
                            images.append({
                                'url': img_url,
                                'alt': img.get_attribute('alt') or '',
                                'width': img.get_attribute('width') or '0',
                                'height': img.get_attribute('height') or '0',
                                'class': img.get_attribute('class') or '',
                                'source': 'img_tag'
                            })
                            seen_urls.add(img_url)
                            
                except Exception as e:
                    continue
            
            # 2. ä»é¡µé¢æºç ä¸­æå–å›¾ç‰‡URLï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            try:
                page_source = self.driver.page_source
                
                # é˜¿é‡Œäº‘å›¾ç‰‡URLæ¨¡å¼
                alicdn_patterns = [
                    r'https://[^"\'\s]*\.alicdn\.com[^"\'\s]*\.(?:jpg|jpeg|png|webp|gif)',
                    r'https://cbu[^"\'\s]*\.alicdn\.com[^"\'\s]*\.(?:jpg|jpeg|png|webp|gif)',
                    r'https://img[^"\'\s]*\.alicdn\.com[^"\'\s]*\.(?:jpg|jpeg|png|webp|gif)'
                ]
                
                for pattern in alicdn_patterns:
                    matches = re.findall(pattern, page_source, re.IGNORECASE)
                    for url in matches:
                        if url not in seen_urls and self.is_valid_product_image_url(url):
                            images.append({
                                'url': url,
                                'alt': '',
                                'width': '0',
                                'height': '0',
                                'class': '',
                                'source': 'regex_extract'
                            })
                            seen_urls.add(url)
                            
            except Exception as e:
                print(f"âŒ æ­£åˆ™æå–å›¾ç‰‡å¤±è´¥: {e}")
            
            # 3. æŸ¥æ‰¾ç‰¹å®šçš„å•†å“å›¾ç‰‡å®¹å™¨
            image_containers = [
                '.offer-img', '.product-img', '.detail-img', '.gallery-img',
                '[class*="image"]', '[class*="photo"]', '[class*="pic"]',
                '.img-list', '.image-list', '.photo-list'
            ]
            
            for container_selector in image_containers:
                try:
                    containers = self.driver.find_elements(By.CSS_SELECTOR, container_selector)
                    for container in containers:
                        container_imgs = container.find_elements(By.TAG_NAME, "img")
                        for img in container_imgs:
                            img_url = self.get_best_image_url(img)
                            if img_url and img_url not in seen_urls:
                                images.append({
                                    'url': img_url,
                                    'alt': img.get_attribute('alt') or '',
                                    'width': img.get_attribute('width') or '0',
                                    'height': img.get_attribute('height') or '0',
                                    'class': img.get_attribute('class') or '',
                                    'source': f'container_{container_selector}'
                                })
                                seen_urls.add(img_url)
                except:
                    continue
            
            # 4. æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå›¾ç‰‡
            self.scroll_to_load_images()
            time.sleep(2)
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„å›¾ç‰‡åŠ è½½
            new_img_elements = self.driver.find_elements(By.TAG_NAME, "img")
            for img in new_img_elements:
                try:
                    img_url = self.get_best_image_url(img)
                    if img_url and img_url not in seen_urls and self.is_product_image(img_url, img):
                        images.append({
                            'url': img_url,
                            'alt': img.get_attribute('alt') or '',
                            'width': img.get_attribute('width') or '0',
                            'height': img.get_attribute('height') or '0',
                            'class': img.get_attribute('class') or '',
                            'source': 'lazy_load'
                        })
                        seen_urls.add(img_url)
                except:
                    continue
            
            if images:
                print(f"âœ… æå–åˆ° {len(images)} å¼ å•†å“å›¾ç‰‡")
                # æŒ‰å›¾ç‰‡è´¨é‡æ’åºï¼ˆä¼˜å…ˆé€‰æ‹©å¤§å°ºå¯¸å›¾ç‰‡ï¼‰
                images = self.sort_images_by_quality(images)
                return images
            else:
                print("âŒ æœªæ‰¾åˆ°å•†å“å›¾ç‰‡")
                return []
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡æå–å¤±è´¥: {e}")
            return []
    
    def get_best_image_url(self, img_element):
        """è·å–å›¾ç‰‡å…ƒç´ çš„æœ€ä½³URL"""
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒå±æ€§
        attributes = ['data-original', 'data-src', 'data-lazy', 'src', 'data-img', 'data-url']
        
        for attr in attributes:
            try:
                url = img_element.get_attribute(attr)
                if url and url.startswith('http'):
                    # ä¼˜å…ˆé€‰æ‹©é«˜æ¸…å›¾ç‰‡
                    if '_b.jpg' in url or '_large' in url or '_big' in url:
                        return url
                    elif any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
                        return url
                    elif 'alicdn.com' in url:
                        return url
            except:
                continue
        return None
    
    def is_product_image(self, img_url, img_element=None):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå•†å“ç›¸å…³å›¾ç‰‡"""
        # æ’é™¤æ˜æ˜¾çš„è£…é¥°å›¾ç‰‡å’Œå›¾æ ‡
        exclude_patterns = [
            'icon', 'logo', 'btn', 'button', 'arrow', 'star', 'rating',
            'header', 'footer', 'nav', 'menu', 'banner', 'ad',
            'sprite', 'background', 'bg', 'decoration'
        ]
        
        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«æ’é™¤æ¨¡å¼
        url_lower = img_url.lower()
        if any(pattern in url_lower for pattern in exclude_patterns):
            return False
        
        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if img_element:
            try:
                width = int(img_element.get_attribute('width') or 0)
                height = int(img_element.get_attribute('height') or 0)
                
                # æ’é™¤å¤ªå°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å›¾æ ‡ï¼‰
                if width > 0 and height > 0 and (width < 50 or height < 50):
                    return False
                    
                # æ’é™¤æ˜æ˜¾çš„è£…é¥°å›¾ç‰‡å°ºå¯¸
                if width > 0 and height > 0 and width * height < 2500:  # 50x50ä»¥ä¸‹
                    return False
            except:
                pass
        
        # ä¼˜å…ˆé€‰æ‹©é˜¿é‡Œäº‘å›¾ç‰‡æœåŠ¡çš„å›¾ç‰‡
        if 'alicdn.com' in img_url:
            return True
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§å›¾ç‰‡æ ¼å¼
        if any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            return True
            
        return False
    
    def is_valid_product_image_url(self, url):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å•†å“å›¾ç‰‡URL"""
        if not url or len(url) < 20:
            return False
            
        # å¿…é¡»æ˜¯HTTPS
        if not url.startswith('https://'):
            return False
            
        # å¿…é¡»åŒ…å«å›¾ç‰‡æ‰©å±•åæˆ–é˜¿é‡Œäº‘åŸŸå
        url_lower = url.lower()
        if not (any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']) or 'alicdn.com' in url_lower):
            return False
            
        # æ’é™¤æ˜æ˜¾çš„éå•†å“å›¾ç‰‡
        exclude_keywords = ['icon', 'logo', 'btn', 'arrow', 'star', 'sprite']
        if any(keyword in url_lower for keyword in exclude_keywords):
            return False
            
        return True
    
    def scroll_to_load_images(self):
        """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‡’åŠ è½½çš„å›¾ç‰‡"""
        try:
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            
            # æ»šåŠ¨åˆ°é¡µé¢é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # åˆ†æ®µæ»šåŠ¨
            viewport_height = self.driver.execute_script("return window.innerHeight")
            page_height = self.driver.execute_script("return document.body.scrollHeight")
            
            current_position = 0
            while current_position < page_height:
                self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                time.sleep(0.5)
                current_position += viewport_height
                
        except Exception as e:
            print(f"âŒ æ»šåŠ¨åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
    
    def sort_images_by_quality(self, images):
        """æŒ‰å›¾ç‰‡è´¨é‡æ’åº"""
        def get_image_priority(img):
            url = img['url'].lower()
            priority = 0
            
            # ä¼˜å…ˆçº§åŠ åˆ†é¡¹
            if '_b.jpg' in url or '_large' in url or '_big' in url:
                priority += 100
            if 'cbu01.alicdn.com' in url:
                priority += 50
            if any(dim in url for dim in ['800', '600', '400']):
                priority += 30
            if img['source'] == 'img_tag':
                priority += 20
            
            # æ ¹æ®å°ºå¯¸åŠ åˆ†
            try:
                width = int(img.get('width', 0))
                height = int(img.get('height', 0))
                if width > 200 and height > 200:
                    priority += 40
                elif width > 100 and height > 100:
                    priority += 20
            except:
                pass
                
            return priority
        
        return sorted(images, key=get_image_priority, reverse=True)
    
    def extract_supplier(self):
        """æå–ä¾›åº”å•†ä¿¡æ¯"""
        supplier_selectors = [
            '.company-name', '.supplier-name', '.shop-name',
            '[class*="company"]', '[class*="supplier"]', '[class*="shop"]'
        ]
        
        for selector in supplier_selectors:
            try:
                element = self.driver.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text and len(text) > 2:
                    print(f"âœ… ä¾›åº”å•†: {text}")
                    return text
            except:
                continue
        
        print("âŒ æœªæ‰¾åˆ°ä¾›åº”å•†ä¿¡æ¯")
        return None
    
    def extract_specifications(self):
        """æå–å•†å“è§„æ ¼"""
        specs = {}
        
        try:
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for table in tables:
                rows = table.find_elements(By.TAG_NAME, "tr")
                for row in rows:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 2:
                        key = cells[0].text.strip()
                        value = cells[1].text.strip()
                        if key and value:
                            specs[key] = value
        except:
            pass
        
        if specs:
            print(f"âœ… è§„æ ¼å‚æ•°: {len(specs)} é¡¹")
            return specs
        
        print("âŒ æœªæ‰¾åˆ°è§„æ ¼å‚æ•°")
        return {}
    
    def extract_moq(self):
        """æå–æœ€å°èµ·è®¢é‡"""
        try:
            moq_keywords = ["èµ·è®¢é‡", "æœ€å°", "MOQ", "èµ·æ‰¹"]
            page_text = self.driver.find_element(By.TAG_NAME, "body").text
            
            for keyword in moq_keywords:
                pattern = rf'{keyword}[ï¼š:]\s*(\d+)'
                match = re.search(pattern, page_text)
                if match:
                    moq_value = match.group(1)
                    print(f"âœ… èµ·è®¢é‡: {moq_value}")
                    return moq_value
        except:
            pass
        
        print("âŒ æœªæ‰¾åˆ°èµ·è®¢é‡ä¿¡æ¯")
        return None
    
    def save_single_product(self, product_data, index):
        """ä¿å­˜å•ä¸ªå•†å“æ•°æ®"""
        try:
            filename = f"data/product_{self.session_timestamp}_{index:03d}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(product_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ä¿å­˜å•ä¸ªå•†å“æ•°æ®å¤±è´¥: {e}")
    
    def save_batch_results(self):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            # ä¿å­˜å®Œæ•´JSONæ•°æ®
            json_file = f"batch_results/batch_{self.session_timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.all_products_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ‰¹é‡JSONæ•°æ®å·²ä¿å­˜: {json_file}")
            
            # ä¿å­˜æ±‡æ€»CSV
            csv_file = f"batch_results/batch_summary_{self.session_timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['åºå·', 'URL', 'å•†å“æ ‡é¢˜', 'ä»·æ ¼', 'ä¾›åº”å•†', 'å›¾ç‰‡æ•°é‡', 'è§„æ ¼æ•°é‡'])
                
                for product in self.all_products_data:
                    writer.writerow([
                        product.get('index', ''),
                        product.get('url', ''),
                        product.get('title', ''),
                        str(product.get('price', [])[:2]) if product.get('price') else '',
                        product.get('supplier', ''),
                        len(product.get('images', [])),
                        len(product.get('specifications', {}))
                    ])
            print(f"âœ… æ‰¹é‡CSVæ±‡æ€»å·²ä¿å­˜: {csv_file}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ‰¹é‡ç»“æœå¤±è´¥: {e}")
    
    def print_summary(self, successful_count, total_urls, failed_urls):
        """æ‰“å°å¤„ç†ç»“æœæ±‡æ€»"""
        print(f"\n" + "="*80)
        print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»")
        print("="*80)
        print(f"æ€»é“¾æ¥æ•°: {total_urls}")
        print(f"æˆåŠŸå¤„ç†: {successful_count}")
        print(f"å¤„ç†å¤±è´¥: {len(failed_urls)}")
        print(f"æˆåŠŸç‡: {(successful_count/total_urls)*100:.1f}%")
        
        if failed_urls:
            print(f"\nâŒ å¤±è´¥çš„é“¾æ¥:")
            for index, url in failed_urls:
                print(f"  {index}. {url[:60]}...")
        
        print("="*80)
    
    def download_all_images(self):
        """ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡"""
        if not self.all_products_data:
            return
        
        print(f"\nğŸ“¸ å¼€å§‹ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡...")
        
        for product in self.all_products_data:
            if product.get('images'):
                index = product.get('index', 0)
                self.download_product_images(product['images'], index)
    
    def download_product_images(self, images_data, product_index):
        """ä¸‹è½½å•ä¸ªå•†å“çš„æ‰€æœ‰å›¾ç‰‡"""
        if not images_data:
            print(f"âŒ å•†å“ {product_index} æ²¡æœ‰å›¾ç‰‡å¯ä¸‹è½½")
            return
            
        print(f"ğŸ“¸ å¼€å§‹ä¸‹è½½å•†å“ {product_index} çš„ {len(images_data)} å¼ å›¾ç‰‡...")
        
        downloaded_count = 0
        failed_count = 0
        
        for i, img_data in enumerate(images_data):  # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
            try:
                img_url = img_data['url']
                
                # å‘é€è¯·æ±‚ä¸‹è½½å›¾ç‰‡
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://detail.1688.com/',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8'
                }
                
                response = requests.get(img_url, timeout=15, headers=headers)
                
                if response.status_code == 200:
                    # è·å–æ–‡ä»¶æ‰©å±•å
                    ext = self.get_image_extension(img_url, response)
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    source = img_data.get('source', 'unknown')
                    filename = f"images/product_{product_index:03d}_{i+1:02d}_{source}.{ext}"
                    
                    # ä¿å­˜å›¾ç‰‡
                    with open(filename, 'wb') as f:
                        f.write(response.content)
                    
                    # è·å–å›¾ç‰‡å¤§å°
                    file_size = len(response.content)
                    size_kb = file_size / 1024
                    
                    print(f"âœ… å›¾ç‰‡ {i+1}/{len(images_data)}: {filename} ({size_kb:.1f}KB)")
                    downloaded_count += 1
                    
                    # æ·»åŠ å°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(random.uniform(0.2, 0.5))
                    
                else:
                    print(f"âŒ å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: HTTP {response.status_code}")
                    failed_count += 1
                    
            except Exception as e:
                print(f"âŒ å›¾ç‰‡ {i+1} ä¸‹è½½å¤±è´¥: {e}")
                failed_count += 1
        
        print(f"ğŸ“Š å•†å“ {product_index} å›¾ç‰‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {downloaded_count} å¼ , å¤±è´¥ {failed_count} å¼ ")
    
    def get_image_extension(self, url, response=None):
        """è·å–å›¾ç‰‡æ–‡ä»¶æ‰©å±•å"""
        # é¦–å…ˆä»URLè·å–æ‰©å±•å
        url_lower = url.lower()
        for ext in ['jpg', 'jpeg', 'png', 'webp', 'gif', 'bmp']:
            if f'.{ext}' in url_lower:
                return ext
        
        # å¦‚æœURLä¸­æ²¡æœ‰æ‰©å±•åï¼Œä»å“åº”å¤´è·å–
        if response:
            content_type = response.headers.get('content-type', '').lower()
            if 'jpeg' in content_type or 'jpg' in content_type:
                return 'jpg'
            elif 'png' in content_type:
                return 'png'
            elif 'webp' in content_type:
                return 'webp'
            elif 'gif' in content_type:
                return 'gif'
        
        # é»˜è®¤è¿”å›jpg
        return 'jpg'
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            input("\nğŸ“‹ æ‰¹é‡å¤„ç†å®Œæˆï¼æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            self.driver.quit()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆæ‰¹é‡1688å•†å“ä¿¡æ¯æå–å™¨")
    print("ğŸ“ è‡ªåŠ¨è¯»å– input.txt æ–‡ä»¶ä¸­çš„é“¾æ¥")
    print("="*60)
    
    crawler = None
    try:
        crawler = SimpleBatch1688()
        
        # è¯»å–é“¾æ¥
        urls = crawler.read_urls_from_input()
        
        if not urls:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é“¾æ¥")
            print("è¯·åœ¨ input.txt æ–‡ä»¶ä¸­æ·»åŠ 1688å•†å“é“¾æ¥ï¼Œæ¯è¡Œä¸€ä¸ª")
            return
        
        print(f"\nâœ… å…±æ‰¾åˆ° {len(urls)} ä¸ªæœ‰æ•ˆé“¾æ¥:")
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url[:60]}...")
        
        # ç¡®è®¤å¼€å§‹å¤„ç†
        confirm = input(f"\næ˜¯å¦å¼€å§‹å¤„ç†è¿™ {len(urls)} ä¸ªé“¾æ¥ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        results = crawler.process_all_urls(urls)
        
        if results:
            # è¯¢é—®æ˜¯å¦ä¸‹è½½å›¾ç‰‡
            download_images = input("\næ˜¯å¦ä¸‹è½½æ‰€æœ‰å•†å“å›¾ç‰‡ï¼Ÿ(y/n): ").strip().lower()
            if download_images == 'y':
                crawler.download_all_images()
            
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
            print(f"  - batch_results/ æ–‡ä»¶å¤¹: æ‰¹é‡å¤„ç†ç»“æœ")
            print(f"  - data/ æ–‡ä»¶å¤¹: å•ä¸ªå•†å“JSONæ–‡ä»¶")
            print(f"  - images/ æ–‡ä»¶å¤¹: å•†å“å›¾ç‰‡")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å•†å“")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    
    finally:
        if crawler:
            crawler.close()

if __name__ == "__main__":
    main()
